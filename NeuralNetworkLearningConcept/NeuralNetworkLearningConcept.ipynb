{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# loss function ---> mean squared error\n",
    "\n",
    "import numpy as np\n",
    "def mean_squared_error(y,t):\n",
    "    return (1/2)*(np.sum((y-t)**2))\n",
    "\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "print(mean_squared_error(np.array(y),np.array(t)))\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "print(mean_squared_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "# loss function ---> cross entropy error\n",
    "\n",
    "import numpy as np\n",
    "def cross_entropy_error(y,t):\n",
    "    # need delta because np.log(0)=-inf (delta means a very small number)\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[58670 42595 57788 44902 30371 39010  9976 46163 48363 32590]\n"
     ]
    }
   ],
   "source": [
    "# Little batch learning\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=load_mnist(normalize=True,one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=10\n",
    "batch_mask=np.random.choice(train_size,batch_size)\n",
    "x_batch=x_train[batch_mask]\n",
    "t_batch=t_train[batch_mask]\n",
    "print(np.random.choice(60000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one or batch learning\n",
    "# cross entropy error\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        y=y.reshape(1,y.size)\n",
    "        t=t.reshape(1,t.size)\n",
    "    batch=y.shape[0]\n",
    "    delta=1e-7\n",
    "    return -(1/batch)*np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0 1]\n",
      "[0.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.array([[2],[3]])\n",
    "y = np.array([[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0],[0.1, 0.05, 0.0, 0.7, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]])\n",
    "\n",
    "batch_size = y.shape[0]\n",
    "print(batch_size)\n",
    "print(np.arange(batch_size))\n",
    "\n",
    "print(y[np.array([0]), np.array([2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# numerical differentation\n",
    "# rounding error\n",
    "import numpy as np\n",
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return(f(x+h)-f(x))/h\n",
    "print(np.float32(1e-50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical differentation\n",
    "import numpy as np\n",
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "def function1(x):\n",
    "    return 0.01*x**2+0.1*x\n",
    "x=np.arange(0.0,20.0,0.1)\n",
    "y=function1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "print(numerical_diff(function1,5))\n",
    "print(numerical_diff(function1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "# partial numerical differentation\n",
    "def function2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "def function_temp1(x0):\n",
    "    return x0*x0+4.0**2.0\n",
    "print(numerical_diff(function_temp1,3))\n",
    "def function_temp2(x1):\n",
    "    return 3.0**2.0+x1*x1\n",
    "print(numerical_diff(function_temp2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# gradient\n",
    "import numpy as np\n",
    "def function2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp=x[idx]\n",
    "        x[idx]=tmp+h\n",
    "        fxh1=f(x)\n",
    "        \n",
    "        x[idx]=tmp-h\n",
    "        fxh2=f(x)\n",
    "        grad[idx]=(fxh1-fxh2)/(2*h)\n",
    "        x[idx]=tmp\n",
    "    return grad\n",
    "print(numerical_gradient(function2,np.array([3.0,4.0])))\n",
    "print(numerical_gradient(function2,np.array([0.0,2.0])))\n",
    "print(numerical_gradient(function2,np.array([3.0,0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.11110793e-10 8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "import numpy as np\n",
    "def function2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp=x[idx]\n",
    "        x[idx]=tmp+h\n",
    "        fxh1=f(x)\n",
    "        \n",
    "        x[idx]=tmp-h\n",
    "        fxh2=f(x)\n",
    "        grad[idx]=(fxh1-fxh2)/(2*h)\n",
    "        x[idx]=tmp\n",
    "    return grad\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x\n",
    "print(gradient_descent(function2,np.array([3.0,4.0]),lr=0.1,step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "# lr too large\n",
    "print(gradient_descent(function2,np.array([3.0,4.0]),lr=10.0,step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.99999994 3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "# lr too small\n",
    "print(gradient_descent(function2,np.array([3.0,4.0]),lr=1e-10,step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02737284  0.51224363 -0.53961647]\n",
      " [ 0.04105926  0.76836545 -0.80942471]]\n"
     ]
    }
   ],
   "source": [
    "# simple net\n",
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layer net class\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class twolayernet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net=twolayernet(input_size=784,hidden_size=100,output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09595568 0.11003978 0.09723213 0.1063434  0.09428568 0.10146014\n",
      "  0.10547546 0.10168134 0.08774367 0.09978272]\n",
      " [0.09619192 0.10948416 0.0969677  0.10611441 0.09434388 0.1017556\n",
      "  0.10545318 0.10186663 0.08773837 0.10008415]\n",
      " [0.09624855 0.10942596 0.09676414 0.10611228 0.09445955 0.10191142\n",
      "  0.10526218 0.10177268 0.08818303 0.09986021]\n",
      " [0.09619814 0.10958402 0.09680399 0.10673075 0.09406141 0.10181977\n",
      "  0.10536756 0.10177924 0.08782046 0.09983467]\n",
      " [0.09609143 0.10935968 0.09695593 0.10643619 0.09416967 0.10193103\n",
      "  0.10526377 0.10170353 0.08813566 0.09995311]\n",
      " [0.09660481 0.10950283 0.09700537 0.10643961 0.09421432 0.10165904\n",
      "  0.10510169 0.10164234 0.08799657 0.09983342]\n",
      " [0.09636989 0.10978328 0.09649826 0.10623308 0.09466042 0.10194298\n",
      "  0.10512242 0.10136259 0.08827501 0.09975207]\n",
      " [0.09612138 0.10935792 0.09697778 0.10632568 0.09458079 0.10161735\n",
      "  0.10557895 0.10145888 0.08792822 0.10005307]\n",
      " [0.0962214  0.1096367  0.09653776 0.10645812 0.09446491 0.10169003\n",
      "  0.10551385 0.10124955 0.08809147 0.10013621]\n",
      " [0.09612537 0.10968006 0.09684199 0.10600282 0.09431866 0.10184303\n",
      "  0.10572338 0.10183314 0.08800019 0.09963136]\n",
      " [0.09602118 0.10999033 0.09681797 0.10643574 0.09429007 0.10167344\n",
      "  0.10574254 0.10149514 0.08768364 0.09984996]\n",
      " [0.09603461 0.10954197 0.09692068 0.10637192 0.09466173 0.10152772\n",
      "  0.10542207 0.10152256 0.08812461 0.09987213]\n",
      " [0.0963999  0.10996846 0.09693372 0.10638461 0.09403913 0.10161057\n",
      "  0.10533221 0.10147417 0.08792395 0.09993328]\n",
      " [0.09619364 0.10990946 0.09698782 0.1063035  0.09456198 0.10200967\n",
      "  0.10490341 0.10167743 0.08785506 0.09959802]\n",
      " [0.095943   0.10974942 0.09686637 0.10646181 0.09466619 0.10158285\n",
      "  0.10533947 0.1016024  0.08806679 0.0997217 ]\n",
      " [0.09641108 0.10944421 0.09677034 0.10636029 0.0941482  0.10162307\n",
      "  0.10567168 0.10158009 0.08810683 0.09988421]\n",
      " [0.09611622 0.1095539  0.09705033 0.10624996 0.09424037 0.10129974\n",
      "  0.10593836 0.10175938 0.08797718 0.09981456]\n",
      " [0.09616213 0.10984319 0.09655032 0.10622726 0.0944884  0.10169741\n",
      "  0.10541679 0.10190126 0.08790802 0.09980522]\n",
      " [0.09610179 0.10979635 0.09700053 0.10627705 0.09425379 0.10165277\n",
      "  0.10533512 0.10178215 0.08787032 0.09993012]\n",
      " [0.09620849 0.10939562 0.09667005 0.106477   0.09487811 0.10178955\n",
      "  0.10520822 0.10151894 0.0881173  0.09973673]\n",
      " [0.09652514 0.10981942 0.0968167  0.1064449  0.09438568 0.10147397\n",
      "  0.10553555 0.10143299 0.08780978 0.09975587]\n",
      " [0.09636624 0.10951389 0.09682018 0.10624563 0.09410275 0.10181407\n",
      "  0.10564795 0.10149572 0.08796252 0.10003105]\n",
      " [0.09632018 0.10950997 0.09677223 0.10633459 0.09443921 0.10155529\n",
      "  0.10572961 0.10171186 0.0876632  0.09996384]\n",
      " [0.09665958 0.10979722 0.09683559 0.10618872 0.09445283 0.10147228\n",
      "  0.105392   0.10137386 0.087952   0.09987593]\n",
      " [0.09616951 0.10912937 0.09713431 0.10608508 0.0944991  0.10171039\n",
      "  0.10550991 0.10158034 0.08786981 0.10031217]\n",
      " [0.09601009 0.10994379 0.09682712 0.10623969 0.09435791 0.10177838\n",
      "  0.10555648 0.10156732 0.0879202  0.09979903]\n",
      " [0.09609351 0.10946966 0.09701394 0.10661088 0.09441999 0.10175518\n",
      "  0.10531824 0.10161976 0.08803239 0.09966644]\n",
      " [0.09646226 0.10981261 0.09694839 0.1061383  0.09424391 0.10156658\n",
      "  0.1053825  0.10128365 0.08821668 0.09994512]\n",
      " [0.0960121  0.10955729 0.0969429  0.10614885 0.09445347 0.10173692\n",
      "  0.1057601  0.1014005  0.08795849 0.10002936]\n",
      " [0.09619444 0.10981786 0.09683661 0.10649266 0.09442836 0.1017497\n",
      "  0.10517996 0.10127925 0.08805972 0.09996145]\n",
      " [0.09616375 0.10984044 0.09681767 0.10639335 0.09436055 0.10170397\n",
      "  0.10530896 0.10150836 0.08796465 0.09993829]\n",
      " [0.09652395 0.10978359 0.09649292 0.1060212  0.0945461  0.10175164\n",
      "  0.10588008 0.10100967 0.08800775 0.09998311]\n",
      " [0.0961529  0.10961317 0.09686119 0.10594386 0.09466101 0.10142084\n",
      "  0.10575286 0.10151497 0.08806182 0.10001738]\n",
      " [0.09636259 0.10959747 0.09707718 0.10610662 0.09410978 0.10140998\n",
      "  0.10576802 0.10178216 0.08790662 0.09987956]\n",
      " [0.09632861 0.10927582 0.09675939 0.1062287  0.09475933 0.10149305\n",
      "  0.10541975 0.10177999 0.08795436 0.100001  ]\n",
      " [0.0961478  0.10933742 0.09692112 0.10601455 0.09450808 0.10177189\n",
      "  0.10542493 0.10193294 0.08802909 0.09991219]\n",
      " [0.09627117 0.10948896 0.09691746 0.10628968 0.09439987 0.10186136\n",
      "  0.10551087 0.101322   0.08812149 0.09981714]\n",
      " [0.09613975 0.10972599 0.0970509  0.10593651 0.09424017 0.10159052\n",
      "  0.1057233  0.10170424 0.08797668 0.09991194]\n",
      " [0.09623782 0.1094919  0.09685133 0.10601823 0.09444838 0.10168553\n",
      "  0.1057171  0.10147126 0.08794575 0.1001327 ]\n",
      " [0.09610623 0.10956788 0.09714716 0.10638147 0.09442078 0.10141383\n",
      "  0.1053041  0.10182291 0.08791906 0.09991658]\n",
      " [0.09630866 0.10988604 0.09674612 0.10648072 0.09441471 0.10159951\n",
      "  0.10545737 0.10142453 0.08769942 0.09998292]\n",
      " [0.09598662 0.10951038 0.09689061 0.10622412 0.09463537 0.10152009\n",
      "  0.10552519 0.10201234 0.08799321 0.09970207]\n",
      " [0.09588795 0.11001682 0.09658867 0.10634258 0.09473792 0.10159339\n",
      "  0.10563463 0.10141539 0.08793651 0.09984614]\n",
      " [0.09643355 0.10928979 0.09676303 0.10612065 0.09453163 0.10171808\n",
      "  0.1054637  0.10160329 0.08815893 0.09991734]\n",
      " [0.0961337  0.10930007 0.09682611 0.10654507 0.09434706 0.10166763\n",
      "  0.10554765 0.10192504 0.08790287 0.09980481]\n",
      " [0.09643256 0.10914322 0.09729849 0.106177   0.09433064 0.10161269\n",
      "  0.10555818 0.1017021  0.08778451 0.09996061]\n",
      " [0.09610846 0.10939667 0.09701899 0.10665752 0.09451293 0.10140844\n",
      "  0.10537789 0.10138329 0.08821066 0.09992516]\n",
      " [0.09641761 0.10971136 0.09682234 0.10619604 0.09433953 0.10152741\n",
      "  0.10556535 0.10150965 0.08800018 0.09991053]\n",
      " [0.09610212 0.10975207 0.09684909 0.1065148  0.09452445 0.10213272\n",
      "  0.10519755 0.10152678 0.0878382  0.09956221]\n",
      " [0.09597653 0.10956329 0.09670977 0.10612235 0.09475599 0.10143219\n",
      "  0.10586615 0.10178499 0.08779127 0.09999748]\n",
      " [0.09624476 0.10954595 0.09673001 0.10631776 0.09462607 0.10160795\n",
      "  0.1054768  0.10175463 0.08772406 0.09997202]\n",
      " [0.09608424 0.10979656 0.09693758 0.10630161 0.0942425  0.10166228\n",
      "  0.10565691 0.10167701 0.08808691 0.0995544 ]\n",
      " [0.09599892 0.10988982 0.0967126  0.10626107 0.09434664 0.10193203\n",
      "  0.10534417 0.10178779 0.08788301 0.09984394]\n",
      " [0.09631802 0.10984317 0.09701869 0.1063062  0.09414764 0.10156365\n",
      "  0.10518501 0.10173743 0.08812705 0.09975313]\n",
      " [0.09631167 0.10964235 0.09694346 0.1062282  0.09450807 0.10144805\n",
      "  0.1058511  0.10146756 0.08771826 0.09988127]\n",
      " [0.09630535 0.10960346 0.09711282 0.10615033 0.09429719 0.10189752\n",
      "  0.10544594 0.1014094  0.0880484  0.09972958]\n",
      " [0.09643643 0.10986246 0.09676837 0.10604486 0.09412367 0.10158054\n",
      "  0.1058514  0.10148873 0.08796742 0.09987612]\n",
      " [0.096724   0.10941649 0.09691743 0.106243   0.09436833 0.10154861\n",
      "  0.10542392 0.10174667 0.08758432 0.10002723]\n",
      " [0.09612796 0.10959187 0.09695974 0.10634051 0.09416684 0.10178162\n",
      "  0.10589592 0.1014106  0.08790828 0.09981665]\n",
      " [0.09615897 0.1096988  0.0969161  0.10626046 0.09447312 0.10145635\n",
      "  0.1055634  0.10137334 0.08784645 0.10025301]\n",
      " [0.09629713 0.10962828 0.09677377 0.1068092  0.0943497  0.10147146\n",
      "  0.10559587 0.10149816 0.08791444 0.09966199]\n",
      " [0.09601199 0.109556   0.09699291 0.10601866 0.09481273 0.10174523\n",
      "  0.10567664 0.1013925  0.08800075 0.09979259]\n",
      " [0.09633078 0.10956813 0.09699551 0.10596815 0.09441796 0.10145934\n",
      "  0.10568673 0.1019711  0.08767177 0.09993053]\n",
      " [0.09653742 0.10978826 0.09684426 0.10658612 0.09405848 0.10148614\n",
      "  0.10531828 0.10181353 0.08766729 0.09990021]\n",
      " [0.09630698 0.11008573 0.09686459 0.10630406 0.09440401 0.10176311\n",
      "  0.10510736 0.10168778 0.08781415 0.09966223]\n",
      " [0.09634757 0.10944834 0.0967979  0.10593751 0.09441491 0.10131974\n",
      "  0.10580601 0.10199415 0.08791315 0.10002071]\n",
      " [0.09607162 0.10984432 0.0968635  0.10643247 0.09449014 0.10154689\n",
      "  0.10548628 0.10147377 0.08780032 0.09999069]\n",
      " [0.09631359 0.10944108 0.09701728 0.10633879 0.09410195 0.10173695\n",
      "  0.10538174 0.10175369 0.08813619 0.09977874]\n",
      " [0.09634296 0.10967033 0.0969076  0.10617086 0.09457679 0.1014937\n",
      "  0.10556085 0.10139869 0.08799886 0.09987936]\n",
      " [0.09635754 0.10968271 0.09666788 0.10645236 0.0942178  0.10173577\n",
      "  0.10556625 0.10170501 0.08777333 0.09984136]\n",
      " [0.09592911 0.10946486 0.09686377 0.10636324 0.09459878 0.10170633\n",
      "  0.10525439 0.10177259 0.08807792 0.09996901]\n",
      " [0.09616413 0.10956396 0.09711598 0.10599203 0.09447323 0.10144199\n",
      "  0.10553869 0.10183925 0.08790964 0.09996109]\n",
      " [0.09623697 0.10915995 0.09689864 0.10667343 0.09453037 0.10172239\n",
      "  0.10531475 0.10186405 0.08793948 0.09965997]\n",
      " [0.0959981  0.10970064 0.09669091 0.10603527 0.09470115 0.1016401\n",
      "  0.10587244 0.10160026 0.08774918 0.10001196]\n",
      " [0.09612596 0.11007215 0.09650754 0.10643583 0.09439557 0.10150523\n",
      "  0.10569903 0.1014097  0.08809896 0.09975004]\n",
      " [0.09614459 0.10970482 0.09653738 0.10612477 0.09439001 0.10202262\n",
      "  0.10556667 0.10184752 0.08786431 0.09979732]\n",
      " [0.09617837 0.10946496 0.0971701  0.10590798 0.09434158 0.10162066\n",
      "  0.10565457 0.10152264 0.08797261 0.10016652]\n",
      " [0.09612507 0.109777   0.09668483 0.10632232 0.0944569  0.10197092\n",
      "  0.10556029 0.101417   0.0879039  0.09978179]\n",
      " [0.09615086 0.10977927 0.09650735 0.10603615 0.09473076 0.10197805\n",
      "  0.1053284  0.10173069 0.08792224 0.09983622]\n",
      " [0.09629408 0.10930201 0.09721173 0.10650304 0.09428999 0.10186063\n",
      "  0.10515522 0.10159823 0.08792179 0.09986328]\n",
      " [0.09614596 0.10968523 0.09681653 0.10634027 0.09436626 0.10168152\n",
      "  0.10562705 0.10168448 0.08770783 0.09994488]\n",
      " [0.09621454 0.10998644 0.09685266 0.10630598 0.09435163 0.10165412\n",
      "  0.10560236 0.10140473 0.08779824 0.0998293 ]\n",
      " [0.09613667 0.10998286 0.09691314 0.10633948 0.09426746 0.1015235\n",
      "  0.10543796 0.10176305 0.08791258 0.0997233 ]\n",
      " [0.09583214 0.11020234 0.09695724 0.10624816 0.09453099 0.1013415\n",
      "  0.10573011 0.1012591  0.08816528 0.09973314]\n",
      " [0.09630123 0.10946603 0.09703033 0.10626286 0.0945426  0.10162572\n",
      "  0.10527678 0.10136618 0.08817438 0.0999539 ]\n",
      " [0.09622336 0.10971151 0.09699584 0.10641391 0.09438164 0.10179987\n",
      "  0.10523392 0.10141335 0.08809105 0.09973553]\n",
      " [0.09599221 0.10967805 0.09705086 0.10647674 0.09415247 0.10166754\n",
      "  0.10548241 0.10150594 0.08793365 0.10006014]\n",
      " [0.09617627 0.10919038 0.09690905 0.10602103 0.09462905 0.10167305\n",
      "  0.10572498 0.10173646 0.0881699  0.09976984]\n",
      " [0.09594613 0.10968917 0.09711544 0.10611345 0.09436335 0.10152888\n",
      "  0.1056693  0.10177351 0.08792015 0.09988061]\n",
      " [0.09625024 0.10991653 0.09685353 0.10655296 0.0944425  0.10149545\n",
      "  0.10550056 0.10142892 0.08803145 0.09952787]\n",
      " [0.09603652 0.10980862 0.09676641 0.10624857 0.094177   0.10158413\n",
      "  0.10545196 0.10190757 0.0880774  0.09994183]\n",
      " [0.09616709 0.10952423 0.09708016 0.10624315 0.0941672  0.10172402\n",
      "  0.10524767 0.10210961 0.08773322 0.10000363]\n",
      " [0.09656906 0.10941277 0.09636859 0.10632991 0.09439654 0.10192758\n",
      "  0.10544018 0.10144117 0.08821209 0.09990211]\n",
      " [0.09613382 0.10934317 0.09675854 0.1063648  0.09472574 0.10176094\n",
      "  0.10554507 0.10167675 0.0878197  0.09987147]\n",
      " [0.09597114 0.10958557 0.09689345 0.10619057 0.09467022 0.1016252\n",
      "  0.10557177 0.10138884 0.08802139 0.10008184]\n",
      " [0.09628199 0.10949824 0.0969813  0.10621325 0.09418033 0.10173511\n",
      "  0.10558645 0.10160858 0.08802887 0.09988589]\n",
      " [0.0962761  0.11000428 0.09673372 0.10649958 0.09445902 0.10188645\n",
      "  0.10529698 0.10130643 0.0876634  0.09987404]\n",
      " [0.096218   0.10933366 0.09706674 0.10618107 0.09454472 0.1015048\n",
      "  0.10570437 0.10164102 0.08795716 0.09984846]\n",
      " [0.09632998 0.10941233 0.09686065 0.10639757 0.09444706 0.10147098\n",
      "  0.10537292 0.10167538 0.08829951 0.09973361]\n",
      " [0.09630871 0.10971743 0.09679574 0.10620986 0.09437899 0.10170401\n",
      "  0.10545148 0.10142705 0.0878414  0.10016532]]\n"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(100,784)\n",
    "y=net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(10,784)\n",
    "t=np.random.rand(10,10)\n",
    "grads=net.numerical_gradient(x,t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train two layer net\n",
    "# random little batch ---> stochastic gradient descent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = twolayernet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 200\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        print(i   )\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
