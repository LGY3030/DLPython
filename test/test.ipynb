{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrain():\n",
    "  train = pd.read_csv(\"S&P 500.csv\")\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "  train[\"date\"] = train[\"Date\"].dt.day\n",
    "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "  train = train.drop([\"Date\"], axis=1)\n",
    "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "  return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 208us/step - loss: 0.0720 - val_loss: 0.0532\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0424 - val_loss: 0.0289\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 0.0203 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0014 - val_loss: 6.0832e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.3052e-04 - val_loss: 3.1527e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.8945e-04 - val_loss: 2.4121e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3636e-04 - val_loss: 2.0389e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.0686e-04 - val_loss: 1.8375e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.8888e-04 - val_loss: 1.7114e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.7603e-04 - val_loss: 1.6146e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.6600e-04 - val_loss: 1.5343e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.5689e-04 - val_loss: 1.4682e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.4880e-04 - val_loss: 1.3973e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 1.4104e-04 - val_loss: 1.3346e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.3409e-04 - val_loss: 1.2669e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.2689e-04 - val_loss: 1.2056e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.2086e-04 - val_loss: 1.1470e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.1431e-04 - val_loss: 1.0941e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.0809e-04 - val_loss: 1.0388e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.0244e-04 - val_loss: 9.8954e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 9.7210e-05 - val_loss: 9.4640e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 9.2316e-05 - val_loss: 9.0227e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 8.7593e-05 - val_loss: 8.5769e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 8.3492e-05 - val_loss: 8.1889e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.9396e-05 - val_loss: 7.8397e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.5610e-05 - val_loss: 7.5121e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 7.2361e-05 - val_loss: 7.2054e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 6.9161e-05 - val_loss: 6.9297e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 6.6484e-05 - val_loss: 6.6766e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 6.3609e-05 - val_loss: 6.4467e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 6.1348e-05 - val_loss: 6.2601e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 5.9246e-05 - val_loss: 6.0601e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.7450e-05 - val_loss: 5.8975e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 5.5629e-05 - val_loss: 5.7236e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.4078e-05 - val_loss: 5.5949e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.2591e-05 - val_loss: 5.4782e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.1556e-05 - val_loss: 5.3636e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.0408e-05 - val_loss: 5.2796e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.9351e-05 - val_loss: 5.2394e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.8723e-05 - val_loss: 5.1398e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.7744e-05 - val_loss: 5.0442e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6906e-05 - val_loss: 5.0083e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.6398e-05 - val_loss: 4.9375e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.5668e-05 - val_loss: 4.8430e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.5100e-05 - val_loss: 4.7768e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.4635e-05 - val_loss: 4.7339e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.4196e-05 - val_loss: 4.6929e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.3768e-05 - val_loss: 4.6275e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3225e-05 - val_loss: 4.6253e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.2973e-05 - val_loss: 4.5741e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 4.2411e-05 - val_loss: 4.5121e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1996e-05 - val_loss: 4.4423e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1669e-05 - val_loss: 4.4333e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.1325e-05 - val_loss: 4.3998e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.0916e-05 - val_loss: 4.3562e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.0749e-05 - val_loss: 4.3218e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 4.0285e-05 - val_loss: 4.2970e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9992e-05 - val_loss: 4.2051e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.9718e-05 - val_loss: 4.1937e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.9288e-05 - val_loss: 4.1472e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9290e-05 - val_loss: 4.1496e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.8583e-05 - val_loss: 4.0690e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.8533e-05 - val_loss: 4.0596e-05\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7959e-05 - val_loss: 4.0294e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.7873e-05 - val_loss: 3.9687e-05\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.7505e-05 - val_loss: 3.9341e-05\n",
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7144e-05 - val_loss: 3.9531e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.6962e-05 - val_loss: 3.8479e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.6638e-05 - val_loss: 3.8017e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6216e-05 - val_loss: 3.8852e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.6190e-05 - val_loss: 3.7556e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.5755e-05 - val_loss: 3.7283e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.5150e-05 - val_loss: 3.7096e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.4991e-05 - val_loss: 3.6392e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.5187e-05 - val_loss: 3.6545e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.4443e-05 - val_loss: 3.6546e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.4003e-05 - val_loss: 3.5526e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.3833e-05 - val_loss: 3.5202e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.3470e-05 - val_loss: 3.4793e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.3429e-05 - val_loss: 3.5265e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2957e-05 - val_loss: 3.4283e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2706e-05 - val_loss: 3.4448e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.2709e-05 - val_loss: 3.3384e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.2118e-05 - val_loss: 3.3832e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 3.2034e-05 - val_loss: 3.2779e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1800e-05 - val_loss: 3.2900e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.1589e-05 - val_loss: 3.2678e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.1412e-05 - val_loss: 3.1767e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.0899e-05 - val_loss: 3.2211e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.0998e-05 - val_loss: 3.1913e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.0349e-05 - val_loss: 3.1408e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.0231e-05 - val_loss: 3.0817e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.0602e-05 - val_loss: 3.1245e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.9722e-05 - val_loss: 3.0959e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9435e-05 - val_loss: 2.9975e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.9359e-05 - val_loss: 2.9800e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.8990e-05 - val_loss: 2.9439e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.9128e-05 - val_loss: 2.9393e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.8675e-05 - val_loss: 2.9296e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8082e-05 - val_loss: 2.8928e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.8032e-05 - val_loss: 2.8745e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.8054e-05 - val_loss: 2.8647e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.7747e-05 - val_loss: 2.7918e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7670e-05 - val_loss: 2.7863e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7471e-05 - val_loss: 2.7564e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7337e-05 - val_loss: 2.8126e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7804e-05 - val_loss: 2.7002e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7103e-05 - val_loss: 2.7477e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6724e-05 - val_loss: 2.7300e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6494e-05 - val_loss: 2.6508e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6668e-05 - val_loss: 2.7895e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.7185e-05 - val_loss: 2.6130e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6220e-05 - val_loss: 2.6912e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.6372e-05 - val_loss: 2.6193e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5699e-05 - val_loss: 2.5691e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5635e-05 - val_loss: 2.5571e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5460e-05 - val_loss: 2.5139e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5397e-05 - val_loss: 2.5901e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5664e-05 - val_loss: 2.6087e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.5633e-05 - val_loss: 2.5794e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.5739e-05 - val_loss: 2.5946e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5422e-05 - val_loss: 2.4694e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4906e-05 - val_loss: 2.5337e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5688e-05 - val_loss: 2.4481e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5014e-05 - val_loss: 2.5412e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4674e-05 - val_loss: 2.4820e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5278e-05 - val_loss: 2.4174e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4576e-05 - val_loss: 2.4364e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4805e-05 - val_loss: 2.5858e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4685e-05 - val_loss: 2.4256e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4535e-05 - val_loss: 2.3943e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4599e-05 - val_loss: 2.3769e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4445e-05 - val_loss: 2.3771e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4342e-05 - val_loss: 2.4015e-05\n",
      "Epoch 136/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4288e-05 - val_loss: 2.3792e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4664e-05 - val_loss: 2.3291e-05\n",
      "Epoch 138/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4515e-05 - val_loss: 2.3675e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4261e-05 - val_loss: 2.3701e-05\n",
      "Epoch 140/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4300e-05 - val_loss: 2.3774e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4422e-05 - val_loss: 2.3602e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4326e-05 - val_loss: 2.3895e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3925e-05 - val_loss: 2.3472e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3820e-05 - val_loss: 2.2931e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3922e-05 - val_loss: 2.2983e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3878e-05 - val_loss: 2.3387e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4160e-05 - val_loss: 2.2624e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4275e-05 - val_loss: 2.3128e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3715e-05 - val_loss: 2.3894e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3879e-05 - val_loss: 2.3308e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3567e-05 - val_loss: 2.2979e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3987e-05 - val_loss: 2.2633e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3502e-05 - val_loss: 2.2880e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3487e-05 - val_loss: 2.3202e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3945e-05 - val_loss: 2.8489e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4438e-05 - val_loss: 2.2935e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3865e-05 - val_loss: 2.3807e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4060e-05 - val_loss: 2.3964e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3575e-05 - val_loss: 2.2360e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3600e-05 - val_loss: 2.3716e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3903e-05 - val_loss: 2.4799e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3972e-05 - val_loss: 2.2759e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3380e-05 - val_loss: 2.2683e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3783e-05 - val_loss: 2.3647e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3682e-05 - val_loss: 2.3120e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3300e-05 - val_loss: 2.2197e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3708e-05 - val_loss: 2.1965e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3373e-05 - val_loss: 2.3912e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3904e-05 - val_loss: 2.6162e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4300e-05 - val_loss: 2.3689e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3890e-05 - val_loss: 2.2342e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - ETA: 0s - loss: 2.0012e-0 - 0s 7us/step - loss: 2.3091e-05 - val_loss: 2.2146e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4391e-05 - val_loss: 2.2941e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.3557e-05 - val_loss: 2.2428e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3443e-05 - val_loss: 2.6578e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4084e-05 - val_loss: 2.2637e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3751e-05 - val_loss: 2.2346e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3123e-05 - val_loss: 2.3356e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3888e-05 - val_loss: 2.2597e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3343e-05 - val_loss: 2.2791e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3470e-05 - val_loss: 2.3030e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3923e-05 - val_loss: 2.2015e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xae07c1e898>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 1s 214us/step - loss: 0.1260 - val_loss: 0.0489\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 0.0198 - val_loss: 0.0065\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 0.0011 - val_loss: 8.5677e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 6.9660e-04 - val_loss: 5.9089e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 5.1024e-04 - val_loss: 4.5863e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.0375e-04 - val_loss: 3.7387e-04\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.3288e-04 - val_loss: 3.1425e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8117e-04 - val_loss: 2.7195e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.4399e-04 - val_loss: 2.3487e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.1421e-04 - val_loss: 2.0926e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 1.9153e-04 - val_loss: 1.8778e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 1.7182e-04 - val_loss: 1.7380e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 1.5650e-04 - val_loss: 1.5775e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 1.4223e-04 - val_loss: 1.4325e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 1.3020e-04 - val_loss: 1.3237e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 1.2060e-04 - val_loss: 1.2294e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 1.1195e-04 - val_loss: 1.1476e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 1.0500e-04 - val_loss: 1.0735e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 9.8774e-05 - val_loss: 1.0232e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 9.2475e-05 - val_loss: 9.6790e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 8.6938e-05 - val_loss: 9.1257e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 8.2485e-05 - val_loss: 8.6512e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 7.8928e-05 - val_loss: 8.6524e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 7.5399e-05 - val_loss: 7.9340e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 7.1534e-05 - val_loss: 7.6184e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 6.9199e-05 - val_loss: 7.3611e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 6.6475e-05 - val_loss: 7.1566e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 6.5563e-05 - val_loss: 7.0516e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 6.3191e-05 - val_loss: 6.7236e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 6.2098e-05 - val_loss: 6.6135e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 5.9823e-05 - val_loss: 6.4836e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 5.8397e-05 - val_loss: 6.2591e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 5.7169e-05 - val_loss: 6.2209e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.6874e-05 - val_loss: 6.1679e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 5.5110e-05 - val_loss: 6.0974e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 5.4645e-05 - val_loss: 5.9504e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 5.3420e-05 - val_loss: 5.7911e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 5.2839e-05 - val_loss: 5.9166e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 5.2958e-05 - val_loss: 5.6633e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 5.1216e-05 - val_loss: 5.7023e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 5.1335e-05 - val_loss: 5.5529e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 5.0895e-05 - val_loss: 5.5095e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.0389e-05 - val_loss: 5.4173e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.9779e-05 - val_loss: 5.4997e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 4.9531e-05 - val_loss: 5.3936e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.8863e-05 - val_loss: 5.2907e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 4.7413e-05 - val_loss: 5.2088e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.7007e-05 - val_loss: 5.1361e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 4.6963e-05 - val_loss: 5.1185e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.6313e-05 - val_loss: 5.2115e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.5559e-05 - val_loss: 5.0248e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.5503e-05 - val_loss: 5.0205e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.5109e-05 - val_loss: 5.0047e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.5094e-05 - val_loss: 5.0924e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 4.4328e-05 - val_loss: 4.9844e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.4405e-05 - val_loss: 5.0051e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.4550e-05 - val_loss: 4.8221e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.3619e-05 - val_loss: 4.6888e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 4.2894e-05 - val_loss: 4.7102e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 4.2462e-05 - val_loss: 4.7016e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 4.2272e-05 - val_loss: 4.8779e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.2080e-05 - val_loss: 4.9086e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.2589e-05 - val_loss: 4.7309e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.1302e-05 - val_loss: 4.5597e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.1735e-05 - val_loss: 4.8258e-05\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.0844e-05 - val_loss: 4.4955e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 4.1663e-05 - val_loss: 4.5534e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 4.0127e-05 - val_loss: 4.4829e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.0075e-05 - val_loss: 4.3987e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.9069e-05 - val_loss: 4.3706e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.9259e-05 - val_loss: 4.5221e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.9645e-05 - val_loss: 4.3180e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.9591e-05 - val_loss: 4.4760e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.8798e-05 - val_loss: 4.5214e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.8759e-05 - val_loss: 4.2973e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.9080e-05 - val_loss: 4.5251e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.7478e-05 - val_loss: 4.1860e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.7928e-05 - val_loss: 4.2078e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.7906e-05 - val_loss: 4.1801e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.7851e-05 - val_loss: 4.1763e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.7152e-05 - val_loss: 4.2490e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.7221e-05 - val_loss: 4.1198e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.6875e-05 - val_loss: 4.8211e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 3.7391e-05 - val_loss: 4.6224e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 3.6662e-05 - val_loss: 4.0101e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.7189e-05 - val_loss: 4.0682e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.5641e-05 - val_loss: 4.0729e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 3.5405e-05 - val_loss: 3.9158e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.5353e-05 - val_loss: 4.0090e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 3.6652e-05 - val_loss: 3.9099e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.5897e-05 - val_loss: 4.0968e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.5298e-05 - val_loss: 4.0086e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 3.4842e-05 - val_loss: 3.7993e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.4571e-05 - val_loss: 3.8083e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.4420e-05 - val_loss: 3.8288e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.4756e-05 - val_loss: 4.0012e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.4236e-05 - val_loss: 3.9927e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.4564e-05 - val_loss: 3.8923e-05\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.3861e-05 - val_loss: 3.7336e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.3082e-05 - val_loss: 3.6717e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.3453e-05 - val_loss: 3.7031e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.2852e-05 - val_loss: 3.6849e-05\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.3480e-05 - val_loss: 4.3125e-05\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.4540e-05 - val_loss: 3.6756e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.2361e-05 - val_loss: 3.6005e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.4708e-05 - val_loss: 3.8854e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.2220e-05 - val_loss: 3.8851e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 3.2912e-05 - val_loss: 3.6041e-05\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.2009e-05 - val_loss: 3.5242e-05\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 3.2370e-05 - val_loss: 4.1718e-05\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 3.1991e-05 - val_loss: 3.4103e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.2167e-05 - val_loss: 3.4647e-05\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.3148e-05 - val_loss: 3.8482e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.2953e-05 - val_loss: 3.4261e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.2532e-05 - val_loss: 3.6546e-05\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.1907e-05 - val_loss: 4.0026e-05\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.2166e-05 - val_loss: 3.5614e-05\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.1051e-05 - val_loss: 3.6706e-05\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.0843e-05 - val_loss: 3.5125e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.2398e-05 - val_loss: 3.8874e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.0475e-05 - val_loss: 3.4987e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.1190e-05 - val_loss: 3.2989e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.1103e-05 - val_loss: 3.4743e-05\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.1012e-05 - val_loss: 3.3681e-05\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.2173e-05 - val_loss: 3.5050e-05\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.0700e-05 - val_loss: 3.8049e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 0s 85us/step - loss: 3.0028e-05 - val_loss: 3.8418e-05\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.0107e-05 - val_loss: 3.3051e-05\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.9710e-05 - val_loss: 3.3990e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.9635e-05 - val_loss: 3.2480e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.9239e-05 - val_loss: 3.2386e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.0667e-05 - val_loss: 3.5245e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 2.9449e-05 - val_loss: 3.1541e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.8549e-05 - val_loss: 3.1698e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 3.0347e-05 - val_loss: 3.4683e-05\n",
      "Epoch 137/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.9771e-05 - val_loss: 3.2927e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 101us/step - loss: 2.9560e-05 - val_loss: 3.2004e-05\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.8903e-05 - val_loss: 3.6294e-05\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8462e-05 - val_loss: 3.4719e-05\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 2.8346e-05 - val_loss: 3.0932e-05\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 2.9023e-05 - val_loss: 3.2613e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 2.8558e-05 - val_loss: 3.0731e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 2.8508e-05 - val_loss: 3.4878e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8543e-05 - val_loss: 3.0739e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8331e-05 - val_loss: 3.5104e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.0044e-05 - val_loss: 5.1000e-05\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.9133e-05 - val_loss: 3.0130e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8051e-05 - val_loss: 3.1167e-05\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.0894e-05 - val_loss: 3.1360e-05\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8079e-05 - val_loss: 3.3817e-05\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.7823e-05 - val_loss: 3.1055e-05\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.6861e-05 - val_loss: 2.9622e-05\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.9285e-05 - val_loss: 3.3813e-05\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.9191e-05 - val_loss: 3.0013e-05\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.0969e-05 - val_loss: 3.0563e-05\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.7199e-05 - val_loss: 3.0981e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 2.9069e-05 - val_loss: 3.5646e-05\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.7958e-05 - val_loss: 3.0689e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.6587e-05 - val_loss: 3.1774e-05\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 2.6409e-05 - val_loss: 2.8838e-05\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 2.7203e-05 - val_loss: 3.0107e-05\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.6986e-05 - val_loss: 3.2212e-05\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.7197e-05 - val_loss: 3.0353e-05\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.7258e-05 - val_loss: 2.8845e-05\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8477e-05 - val_loss: 3.0415e-05\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.6428e-05 - val_loss: 3.1360e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.8822e-05 - val_loss: 3.4425e-05\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8441e-05 - val_loss: 3.2736e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.7598e-05 - val_loss: 2.9429e-05\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 2.7004e-05 - val_loss: 3.1807e-05\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xae15bd5ac8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.add(RepeatVector(5))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(1, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5706 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5706/5706 [==============================] - 1s 155us/step - loss: 0.0716 - val_loss: 0.0550\n",
      "Epoch 2/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 0.0420 - val_loss: 0.0293\n",
      "Epoch 3/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 0.0200 - val_loss: 0.0113\n",
      "Epoch 4/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 5/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 0.0014 - val_loss: 6.1220e-04\n",
      "Epoch 6/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 4.6401e-04 - val_loss: 3.6010e-04\n",
      "Epoch 7/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 3.3031e-04 - val_loss: 2.9060e-04\n",
      "Epoch 8/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.7833e-04 - val_loss: 2.5390e-04\n",
      "Epoch 9/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.4883e-04 - val_loss: 2.3397e-04\n",
      "Epoch 10/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.3072e-04 - val_loss: 2.1890e-04\n",
      "Epoch 11/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.1772e-04 - val_loss: 2.0771e-04\n",
      "Epoch 12/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 2.0672e-04 - val_loss: 1.9816e-04\n",
      "Epoch 13/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 1.9736e-04 - val_loss: 1.9008e-04\n",
      "Epoch 14/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 1.8882e-04 - val_loss: 1.8209e-04\n",
      "Epoch 15/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 1.8062e-04 - val_loss: 1.7536e-04\n",
      "Epoch 16/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 1.7335e-04 - val_loss: 1.6818e-04\n",
      "Epoch 17/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.6627e-04 - val_loss: 1.6200e-04\n",
      "Epoch 18/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.5896e-04 - val_loss: 1.5519e-04\n",
      "Epoch 19/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.5232e-04 - val_loss: 1.4923e-04\n",
      "Epoch 20/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.4624e-04 - val_loss: 1.4349e-04\n",
      "Epoch 21/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.4033e-04 - val_loss: 1.3830e-04\n",
      "Epoch 22/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.3480e-04 - val_loss: 1.3204e-04\n",
      "Epoch 23/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.2956e-04 - val_loss: 1.2779e-04\n",
      "Epoch 24/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.2470e-04 - val_loss: 1.2253e-04\n",
      "Epoch 25/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.2049e-04 - val_loss: 1.1857e-04\n",
      "Epoch 26/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.1602e-04 - val_loss: 1.1502e-04\n",
      "Epoch 27/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.1227e-04 - val_loss: 1.1106e-04\n",
      "Epoch 28/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0859e-04 - val_loss: 1.0812e-04\n",
      "Epoch 29/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0539e-04 - val_loss: 1.0467e-04\n",
      "Epoch 30/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0224e-04 - val_loss: 1.0176e-04\n",
      "Epoch 31/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.9532e-05 - val_loss: 9.9565e-05\n",
      "Epoch 32/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 9.7422e-05 - val_loss: 9.7539e-05\n",
      "Epoch 33/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.5051e-05 - val_loss: 9.6207e-05\n",
      "Epoch 34/1000\n",
      "5706/5706 [==============================] - 0s 7us/step - loss: 9.3154e-05 - val_loss: 9.3727e-05\n",
      "Epoch 35/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.1358e-05 - val_loss: 9.1579e-05\n",
      "Epoch 36/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.9661e-05 - val_loss: 8.9483e-05\n",
      "Epoch 37/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 8.8253e-05 - val_loss: 8.8325e-05\n",
      "Epoch 38/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 8.6773e-05 - val_loss: 8.7263e-05\n",
      "Epoch 39/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.5961e-05 - val_loss: 8.5860e-05\n",
      "Epoch 40/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 8.4780e-05 - val_loss: 8.5320e-05\n",
      "Epoch 41/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.3763e-05 - val_loss: 8.3964e-05\n",
      "Epoch 42/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.2891e-05 - val_loss: 8.3513e-05\n",
      "Epoch 43/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.2296e-05 - val_loss: 8.2399e-05\n",
      "Epoch 44/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 8.1494e-05 - val_loss: 8.1558e-05\n",
      "Epoch 45/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 8.1064e-05 - val_loss: 8.0860e-05\n",
      "Epoch 46/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.0474e-05 - val_loss: 8.1549e-05\n",
      "Epoch 47/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 7.9689e-05 - val_loss: 7.9534e-05\n",
      "Epoch 48/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.9329e-05 - val_loss: 7.9885e-05\n",
      "Epoch 49/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.8578e-05 - val_loss: 7.9197e-05\n",
      "Epoch 50/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.8564e-05 - val_loss: 7.8259e-05\n",
      "Epoch 51/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.7856e-05 - val_loss: 7.7574e-05\n",
      "Epoch 52/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.7854e-05 - val_loss: 7.7029e-05\n",
      "Epoch 53/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.7237e-05 - val_loss: 7.6364e-05\n",
      "Epoch 54/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6976e-05 - val_loss: 7.6420e-05\n",
      "Epoch 55/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6391e-05 - val_loss: 7.6121e-05\n",
      "Epoch 56/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6447e-05 - val_loss: 7.5834e-05\n",
      "Epoch 57/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.5920e-05 - val_loss: 7.5011e-05\n",
      "Epoch 58/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.5218e-05 - val_loss: 7.4296e-05\n",
      "Epoch 59/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.4899e-05 - val_loss: 7.3967e-05\n",
      "Epoch 60/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.4579e-05 - val_loss: 7.4151e-05\n",
      "Epoch 61/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.3963e-05 - val_loss: 7.4279e-05\n",
      "Epoch 62/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.3960e-05 - val_loss: 7.3338e-05\n",
      "Epoch 63/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.3581e-05 - val_loss: 7.3824e-05\n",
      "Epoch 64/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.3202e-05 - val_loss: 7.2108e-05\n",
      "Epoch 65/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.2909e-05 - val_loss: 7.2221e-05\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.2693e-05 - val_loss: 7.1584e-05\n",
      "Epoch 67/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.2095e-05 - val_loss: 7.2374e-05\n",
      "Epoch 68/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.1902e-05 - val_loss: 7.0978e-05\n",
      "Epoch 69/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 7.1651e-05 - val_loss: 7.0416e-05\n",
      "Epoch 70/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 7.1334e-05 - val_loss: 6.9437e-05\n",
      "Epoch 71/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.0922e-05 - val_loss: 6.9515e-05\n",
      "Epoch 72/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.0289e-05 - val_loss: 7.0556e-05\n",
      "Epoch 73/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.0702e-05 - val_loss: 6.9047e-05\n",
      "Epoch 74/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.0314e-05 - val_loss: 6.9656e-05\n",
      "Epoch 75/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.9710e-05 - val_loss: 6.8239e-05\n",
      "Epoch 76/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.9070e-05 - val_loss: 6.8058e-05\n",
      "Epoch 77/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.9101e-05 - val_loss: 6.7379e-05\n",
      "Epoch 78/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.8438e-05 - val_loss: 6.7327e-05\n",
      "Epoch 79/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8035e-05 - val_loss: 6.7007e-05\n",
      "Epoch 80/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.8187e-05 - val_loss: 6.6684e-05\n",
      "Epoch 81/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.8452e-05 - val_loss: 6.6323e-05\n",
      "Epoch 82/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7561e-05 - val_loss: 6.6741e-05\n",
      "Epoch 83/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.7534e-05 - val_loss: 6.5871e-05\n",
      "Epoch 84/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7329e-05 - val_loss: 6.4801e-05\n",
      "Epoch 85/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 6.6763e-05 - val_loss: 6.4425e-05\n",
      "Epoch 86/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 6.6569e-05 - val_loss: 6.7007e-05\n",
      "Epoch 87/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.6069e-05 - val_loss: 6.4608e-05\n",
      "Epoch 88/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.5789e-05 - val_loss: 6.3878e-05\n",
      "Epoch 89/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5418e-05 - val_loss: 6.3542e-05\n",
      "Epoch 90/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.5046e-05 - val_loss: 6.2784e-05\n",
      "Epoch 91/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.5122e-05 - val_loss: 6.2802e-05\n",
      "Epoch 92/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4295e-05 - val_loss: 6.2131e-05\n",
      "Epoch 93/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4578e-05 - val_loss: 6.4648e-05\n",
      "Epoch 94/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.4582e-05 - val_loss: 6.2643e-05\n",
      "Epoch 95/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3948e-05 - val_loss: 6.2164e-05\n",
      "Epoch 96/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3558e-05 - val_loss: 6.2485e-05\n",
      "Epoch 97/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.3918e-05 - val_loss: 6.2909e-05\n",
      "Epoch 98/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.3297e-05 - val_loss: 6.1267e-05\n",
      "Epoch 99/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2941e-05 - val_loss: 6.2274e-05\n",
      "Epoch 100/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2774e-05 - val_loss: 6.1422e-05\n",
      "Epoch 101/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2618e-05 - val_loss: 6.0146e-05\n",
      "Epoch 102/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2374e-05 - val_loss: 6.1601e-05\n",
      "Epoch 103/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2592e-05 - val_loss: 5.9479e-05\n",
      "Epoch 104/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2236e-05 - val_loss: 5.9596e-05\n",
      "Epoch 105/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2070e-05 - val_loss: 6.0040e-05\n",
      "Epoch 106/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.1270e-05 - val_loss: 5.8907e-05\n",
      "Epoch 107/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.1230e-05 - val_loss: 5.9604e-05\n",
      "Epoch 108/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.1363e-05 - val_loss: 5.8397e-05\n",
      "Epoch 109/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0677e-05 - val_loss: 5.8384e-05\n",
      "Epoch 110/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0992e-05 - val_loss: 5.9514e-05\n",
      "Epoch 111/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0260e-05 - val_loss: 5.8339e-05\n",
      "Epoch 112/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0237e-05 - val_loss: 5.8956e-05\n",
      "Epoch 113/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0131e-05 - val_loss: 5.8309e-05\n",
      "Epoch 114/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9850e-05 - val_loss: 5.7642e-05\n",
      "Epoch 115/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0485e-05 - val_loss: 5.7219e-05\n",
      "Epoch 116/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9671e-05 - val_loss: 5.6821e-05\n",
      "Epoch 117/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9152e-05 - val_loss: 5.7717e-05\n",
      "Epoch 118/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9160e-05 - val_loss: 5.6855e-05\n",
      "Epoch 119/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9222e-05 - val_loss: 5.6840e-05\n",
      "Epoch 120/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9207e-05 - val_loss: 5.8866e-05\n",
      "Epoch 121/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9379e-05 - val_loss: 5.7449e-05\n",
      "Epoch 122/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8798e-05 - val_loss: 5.7933e-05\n",
      "Epoch 123/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8563e-05 - val_loss: 5.6531e-05\n",
      "Epoch 124/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8638e-05 - val_loss: 5.6948e-05\n",
      "Epoch 125/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8041e-05 - val_loss: 5.6084e-05\n",
      "Epoch 126/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8852e-05 - val_loss: 5.6834e-05\n",
      "Epoch 127/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8715e-05 - val_loss: 5.5864e-05\n",
      "Epoch 128/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8045e-05 - val_loss: 5.7035e-05\n",
      "Epoch 129/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8111e-05 - val_loss: 5.5085e-05\n",
      "Epoch 130/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8310e-05 - val_loss: 5.7604e-05\n",
      "Epoch 131/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8344e-05 - val_loss: 5.5731e-05\n",
      "Epoch 132/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8173e-05 - val_loss: 5.5370e-05\n",
      "Epoch 133/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7961e-05 - val_loss: 5.5755e-05\n",
      "Epoch 134/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8091e-05 - val_loss: 5.6608e-05\n",
      "Epoch 135/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9215e-05 - val_loss: 5.6246e-05\n",
      "Epoch 136/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8525e-05 - val_loss: 5.4636e-05\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7315e-05 - val_loss: 5.5001e-05\n",
      "Epoch 138/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7564e-05 - val_loss: 5.7054e-05\n",
      "Epoch 139/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7828e-05 - val_loss: 5.5401e-05\n",
      "Epoch 140/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7567e-05 - val_loss: 5.5924e-05\n",
      "Epoch 141/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7272e-05 - val_loss: 5.6636e-05\n",
      "Epoch 142/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7578e-05 - val_loss: 5.5470e-05\n",
      "Epoch 143/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7082e-05 - val_loss: 5.6563e-05\n",
      "Epoch 144/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7268e-05 - val_loss: 5.5443e-05\n",
      "Epoch 145/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7256e-05 - val_loss: 5.6197e-05\n",
      "Epoch 146/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7055e-05 - val_loss: 5.4794e-05\n",
      "Epoch 147/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7565e-05 - val_loss: 5.9139e-05\n",
      "Epoch 148/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7448e-05 - val_loss: 5.5044e-05\n",
      "Epoch 149/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7212e-05 - val_loss: 5.6844e-05\n",
      "Epoch 150/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7534e-05 - val_loss: 5.8173e-05\n",
      "Epoch 151/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7108e-05 - val_loss: 5.6050e-05\n",
      "Epoch 152/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7384e-05 - val_loss: 5.7065e-05\n",
      "Epoch 153/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7467e-05 - val_loss: 5.5533e-05\n",
      "Epoch 154/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6788e-05 - val_loss: 5.4164e-05\n",
      "Epoch 155/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7151e-05 - val_loss: 5.4623e-05\n",
      "Epoch 156/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7365e-05 - val_loss: 5.5650e-05\n",
      "Epoch 157/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6973e-05 - val_loss: 5.4852e-05\n",
      "Epoch 158/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6678e-05 - val_loss: 5.8014e-05\n",
      "Epoch 159/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7740e-05 - val_loss: 5.4837e-05\n",
      "Epoch 160/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6445e-05 - val_loss: 5.6132e-05\n",
      "Epoch 161/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6679e-05 - val_loss: 5.5087e-05\n",
      "Epoch 162/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6490e-05 - val_loss: 5.4708e-05\n",
      "Epoch 163/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6556e-05 - val_loss: 5.5009e-05\n",
      "Epoch 164/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6807e-05 - val_loss: 5.6080e-05\n",
      "Epoch 165/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6759e-05 - val_loss: 5.3986e-05\n",
      "Epoch 166/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6285e-05 - val_loss: 5.4436e-05\n",
      "Epoch 167/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6224e-05 - val_loss: 5.4412e-05\n",
      "Epoch 168/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.6384e-05 - val_loss: 5.6463e-05\n",
      "Epoch 169/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6837e-05 - val_loss: 5.4095e-05\n",
      "Epoch 170/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6466e-05 - val_loss: 5.5012e-05\n",
      "Epoch 171/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6812e-05 - val_loss: 5.4800e-05\n",
      "Epoch 172/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6753e-05 - val_loss: 5.6232e-05\n",
      "Epoch 173/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7583e-05 - val_loss: 5.4295e-05\n",
      "Epoch 174/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6050e-05 - val_loss: 5.6539e-05\n",
      "Epoch 175/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6160e-05 - val_loss: 5.4089e-05\n",
      "Epoch 176/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 5.6484e-05 - val_loss: 5.5879e-05\n",
      "Epoch 177/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 5.6415e-05 - val_loss: 5.4592e-05\n",
      "Epoch 178/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6385e-05 - val_loss: 5.6371e-05\n",
      "Epoch 179/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.5927e-05 - val_loss: 5.4398e-05\n",
      "Epoch 180/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7255e-05 - val_loss: 5.5391e-05\n",
      "Epoch 181/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6140e-05 - val_loss: 5.4166e-05\n",
      "Epoch 182/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6251e-05 - val_loss: 5.4309e-05\n",
      "Epoch 183/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6679e-05 - val_loss: 5.6945e-05\n",
      "Epoch 184/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8075e-05 - val_loss: 5.5201e-05\n",
      "Epoch 185/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6510e-05 - val_loss: 5.4964e-05\n",
      "Epoch 186/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6536e-05 - val_loss: 5.4867e-05\n",
      "Epoch 187/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6398e-05 - val_loss: 5.4500e-05\n",
      "Epoch 188/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6489e-05 - val_loss: 5.5536e-05\n",
      "Epoch 189/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6504e-05 - val_loss: 5.4519e-05\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xae15927fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(5, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5703 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "5703/5703 [==============================] - 1s 193us/step - loss: 0.0997 - val_loss: 0.0552\n",
      "Epoch 2/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 0.0360 - val_loss: 0.0169\n",
      "Epoch 3/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 0.0125 - val_loss: 0.0087\n",
      "Epoch 4/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 5/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 6/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 0.0054 - val_loss: 0.0046\n",
      "Epoch 7/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 8/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 9/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 10/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 11/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 12/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 13/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 0.0011 - val_loss: 9.6739e-04\n",
      "Epoch 14/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 8.8314e-04 - val_loss: 8.0007e-04\n",
      "Epoch 15/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 7.3567e-04 - val_loss: 6.7170e-04\n",
      "Epoch 16/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 6.2166e-04 - val_loss: 5.7326e-04\n",
      "Epoch 17/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 5.3271e-04 - val_loss: 4.9594e-04\n",
      "Epoch 18/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 4.6203e-04 - val_loss: 4.3487e-04\n",
      "Epoch 19/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 4.0573e-04 - val_loss: 3.8609e-04\n",
      "Epoch 20/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 3.6110e-04 - val_loss: 3.4692e-04\n",
      "Epoch 21/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 3.2542e-04 - val_loss: 3.1397e-04\n",
      "Epoch 22/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 2.9625e-04 - val_loss: 2.8895e-04\n",
      "Epoch 23/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 2.7254e-04 - val_loss: 2.6697e-04\n",
      "Epoch 24/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 2.5273e-04 - val_loss: 2.4779e-04\n",
      "Epoch 25/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 2.3590e-04 - val_loss: 2.3218e-04\n",
      "Epoch 26/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 2.2209e-04 - val_loss: 2.1857e-04\n",
      "Epoch 27/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 2.0971e-04 - val_loss: 2.0677e-04\n",
      "Epoch 28/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.9903e-04 - val_loss: 1.9637e-04\n",
      "Epoch 29/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.8945e-04 - val_loss: 1.8740e-04\n",
      "Epoch 30/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.8139e-04 - val_loss: 1.7957e-04\n",
      "Epoch 31/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.7397e-04 - val_loss: 1.7217e-04\n",
      "Epoch 32/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.6739e-04 - val_loss: 1.6668e-04\n",
      "Epoch 33/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.6176e-04 - val_loss: 1.6030e-04\n",
      "Epoch 34/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.5613e-04 - val_loss: 1.5588e-04\n",
      "Epoch 35/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.5193e-04 - val_loss: 1.5099e-04\n",
      "Epoch 36/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.4743e-04 - val_loss: 1.4712e-04\n",
      "Epoch 37/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.4376e-04 - val_loss: 1.4337e-04\n",
      "Epoch 38/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.4050e-04 - val_loss: 1.4163e-04\n",
      "Epoch 39/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.3730e-04 - val_loss: 1.3700e-04\n",
      "Epoch 40/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.3411e-04 - val_loss: 1.3450e-04\n",
      "Epoch 41/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.3173e-04 - val_loss: 1.3213e-04\n",
      "Epoch 42/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.2925e-04 - val_loss: 1.2983e-04\n",
      "Epoch 43/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.2715e-04 - val_loss: 1.2867e-04\n",
      "Epoch 44/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.2530e-04 - val_loss: 1.2777e-04\n",
      "Epoch 45/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.2333e-04 - val_loss: 1.2454e-04\n",
      "Epoch 46/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.2167e-04 - val_loss: 1.2322e-04\n",
      "Epoch 47/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.2073e-04 - val_loss: 1.2134e-04\n",
      "Epoch 48/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.1877e-04 - val_loss: 1.2033e-04\n",
      "Epoch 49/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1758e-04 - val_loss: 1.1992e-04\n",
      "Epoch 50/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.1625e-04 - val_loss: 1.1864e-04\n",
      "Epoch 51/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.1484e-04 - val_loss: 1.1804e-04\n",
      "Epoch 52/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.1390e-04 - val_loss: 1.1682e-04\n",
      "Epoch 53/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.1321e-04 - val_loss: 1.1519e-04\n",
      "Epoch 54/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.1174e-04 - val_loss: 1.1526e-04\n",
      "Epoch 55/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1096e-04 - val_loss: 1.1426e-04\n",
      "Epoch 56/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 1.1019e-04 - val_loss: 1.1303e-04\n",
      "Epoch 57/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0936e-04 - val_loss: 1.1540e-04\n",
      "Epoch 58/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.0838e-04 - val_loss: 1.1148e-04\n",
      "Epoch 59/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.0751e-04 - val_loss: 1.1050e-04\n",
      "Epoch 60/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0809e-04 - val_loss: 1.1119e-04\n",
      "Epoch 61/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0670e-04 - val_loss: 1.1077e-04\n",
      "Epoch 62/1000\n",
      "5703/5703 [==============================] - 0s 18us/step - loss: 1.0532e-04 - val_loss: 1.0987e-04\n",
      "Epoch 63/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 1.0475e-04 - val_loss: 1.1010e-04\n",
      "Epoch 64/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 1.0399e-04 - val_loss: 1.0803e-04\n",
      "Epoch 65/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0348e-04 - val_loss: 1.0788e-04\n",
      "Epoch 66/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.0309e-04 - val_loss: 1.0716e-04\n",
      "Epoch 67/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 1.0244e-04 - val_loss: 1.0893e-04\n",
      "Epoch 68/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.0266e-04 - val_loss: 1.0648e-04\n",
      "Epoch 69/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.0089e-04 - val_loss: 1.0763e-04\n",
      "Epoch 70/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.0065e-04 - val_loss: 1.0638e-04\n",
      "Epoch 71/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.0002e-04 - val_loss: 1.0516e-04\n",
      "Epoch 72/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.9747e-05 - val_loss: 1.0401e-04\n",
      "Epoch 73/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.9285e-05 - val_loss: 1.0538e-04\n",
      "Epoch 74/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.8778e-05 - val_loss: 1.0456e-04\n",
      "Epoch 75/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.8190e-05 - val_loss: 1.0309e-04\n",
      "Epoch 76/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.7798e-05 - val_loss: 1.0376e-04\n",
      "Epoch 77/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.7184e-05 - val_loss: 1.0312e-04\n",
      "Epoch 78/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.6666e-05 - val_loss: 1.0196e-04\n",
      "Epoch 79/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.6839e-05 - val_loss: 1.0501e-04\n",
      "Epoch 80/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.6219e-05 - val_loss: 1.0171e-04\n",
      "Epoch 81/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.6130e-05 - val_loss: 1.0151e-04\n",
      "Epoch 82/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.5107e-05 - val_loss: 1.0145e-04\n",
      "Epoch 83/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.5246e-05 - val_loss: 1.0239e-04\n",
      "Epoch 84/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.4997e-05 - val_loss: 1.0128e-04\n",
      "Epoch 85/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.4860e-05 - val_loss: 1.0042e-04\n",
      "Epoch 86/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.4226e-05 - val_loss: 1.0063e-04\n",
      "Epoch 87/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.3870e-05 - val_loss: 1.0072e-04\n",
      "Epoch 88/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.3316e-05 - val_loss: 1.0028e-04\n",
      "Epoch 89/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.2999e-05 - val_loss: 9.9838e-05\n",
      "Epoch 90/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.3126e-05 - val_loss: 1.0335e-04\n",
      "Epoch 91/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.2561e-05 - val_loss: 9.9365e-05\n",
      "Epoch 92/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.2286e-05 - val_loss: 9.9373e-05\n",
      "Epoch 93/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.2570e-05 - val_loss: 1.0131e-04\n",
      "Epoch 94/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.2395e-05 - val_loss: 9.9416e-05\n",
      "Epoch 95/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.1354e-05 - val_loss: 9.8882e-05\n",
      "Epoch 96/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.1902e-05 - val_loss: 9.8579e-05\n",
      "Epoch 97/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.1921e-05 - val_loss: 9.8298e-05\n",
      "Epoch 98/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.1317e-05 - val_loss: 9.8930e-05\n",
      "Epoch 99/1000\n",
      "5703/5703 [==============================] - 0s 18us/step - loss: 9.1280e-05 - val_loss: 9.8830e-05\n",
      "Epoch 100/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.0814e-05 - val_loss: 9.9810e-05\n",
      "Epoch 101/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.0789e-05 - val_loss: 9.8879e-05\n",
      "Epoch 102/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.1285e-05 - val_loss: 9.8358e-05\n",
      "Epoch 103/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0563e-05 - val_loss: 9.8705e-05\n",
      "Epoch 104/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.0111e-05 - val_loss: 9.7718e-05\n",
      "Epoch 105/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0225e-05 - val_loss: 9.9969e-05\n",
      "Epoch 106/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.0311e-05 - val_loss: 9.8686e-05\n",
      "Epoch 107/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.0690e-05 - val_loss: 9.8244e-05\n",
      "Epoch 108/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0666e-05 - val_loss: 1.0029e-04\n",
      "Epoch 109/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.0965e-05 - val_loss: 9.7525e-05\n",
      "Epoch 110/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9832e-05 - val_loss: 9.9460e-05\n",
      "Epoch 111/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.1418e-05 - val_loss: 9.8476e-05\n",
      "Epoch 112/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.0986e-05 - val_loss: 9.8250e-05\n",
      "Epoch 113/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9794e-05 - val_loss: 9.6445e-05\n",
      "Epoch 114/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.9780e-05 - val_loss: 9.7711e-05\n",
      "Epoch 115/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.9964e-05 - val_loss: 1.0093e-04\n",
      "Epoch 116/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9044e-05 - val_loss: 9.7563e-05\n",
      "Epoch 117/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 8.8799e-05 - val_loss: 9.6994e-05\n",
      "Epoch 118/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9208e-05 - val_loss: 9.7629e-05\n",
      "Epoch 119/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.0386e-05 - val_loss: 9.6760e-05\n",
      "Epoch 120/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9621e-05 - val_loss: 9.6535e-05\n",
      "Epoch 121/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9424e-05 - val_loss: 9.6977e-05\n",
      "Epoch 122/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.9414e-05 - val_loss: 9.6078e-05\n",
      "Epoch 123/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8748e-05 - val_loss: 1.0012e-04\n",
      "Epoch 124/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.0452e-05 - val_loss: 9.6994e-05\n",
      "Epoch 125/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9671e-05 - val_loss: 9.8324e-05\n",
      "Epoch 126/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8073e-05 - val_loss: 1.0078e-04\n",
      "Epoch 127/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9090e-05 - val_loss: 9.6640e-05\n",
      "Epoch 128/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9548e-05 - val_loss: 9.7844e-05\n",
      "Epoch 129/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9683e-05 - val_loss: 9.5781e-05\n",
      "Epoch 130/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8776e-05 - val_loss: 9.8525e-05\n",
      "Epoch 131/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8568e-05 - val_loss: 9.9141e-05\n",
      "Epoch 132/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 8.8277e-05 - val_loss: 9.5767e-05\n",
      "Epoch 133/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8729e-05 - val_loss: 9.7479e-05\n",
      "Epoch 134/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8578e-05 - val_loss: 9.6766e-05\n",
      "Epoch 135/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9222e-05 - val_loss: 9.6224e-05\n",
      "Epoch 136/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7951e-05 - val_loss: 9.4825e-05\n",
      "Epoch 137/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7949e-05 - val_loss: 9.7544e-05\n",
      "Epoch 138/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7931e-05 - val_loss: 9.6366e-05\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8541e-05 - val_loss: 9.5509e-05\n",
      "Epoch 140/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8916e-05 - val_loss: 9.7078e-05\n",
      "Epoch 141/1000\n",
      "5703/5703 [==============================] - 0s 18us/step - loss: 8.7924e-05 - val_loss: 9.5896e-05\n",
      "Epoch 142/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8650e-05 - val_loss: 9.9337e-05\n",
      "Epoch 143/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7882e-05 - val_loss: 9.6416e-05\n",
      "Epoch 144/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9488e-05 - val_loss: 9.8844e-05\n",
      "Epoch 145/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8746e-05 - val_loss: 9.5390e-05\n",
      "Epoch 146/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 8.8480e-05 - val_loss: 9.5477e-05\n",
      "Epoch 147/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.9817e-05 - val_loss: 9.8696e-05\n",
      "Epoch 148/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8116e-05 - val_loss: 9.7032e-05\n",
      "Epoch 149/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.0644e-05 - val_loss: 9.5195e-05\n",
      "Epoch 150/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 8.7391e-05 - val_loss: 9.5137e-05\n",
      "Epoch 151/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8164e-05 - val_loss: 9.5949e-05\n",
      "Epoch 152/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8513e-05 - val_loss: 9.4172e-05\n",
      "Epoch 153/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.7455e-05 - val_loss: 9.4946e-05\n",
      "Epoch 154/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8619e-05 - val_loss: 9.5684e-05\n",
      "Epoch 155/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7275e-05 - val_loss: 9.4032e-05\n",
      "Epoch 156/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7404e-05 - val_loss: 9.4427e-05\n",
      "Epoch 157/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.7337e-05 - val_loss: 9.8088e-05\n",
      "Epoch 158/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 8.7602e-05 - val_loss: 9.4304e-05\n",
      "Epoch 159/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.6945e-05 - val_loss: 9.5604e-05\n",
      "Epoch 160/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.7958e-05 - val_loss: 9.4858e-05\n",
      "Epoch 161/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8220e-05 - val_loss: 9.5345e-05\n",
      "Epoch 162/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.8407e-05 - val_loss: 9.4339e-05\n",
      "Epoch 163/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9740e-05 - val_loss: 9.9489e-05\n",
      "Epoch 164/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9831e-05 - val_loss: 9.3198e-05\n",
      "Epoch 165/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8531e-05 - val_loss: 9.7943e-05\n",
      "Epoch 166/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8421e-05 - val_loss: 9.7010e-05\n",
      "Epoch 167/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8111e-05 - val_loss: 9.3450e-05\n",
      "Epoch 168/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.7153e-05 - val_loss: 9.3411e-05\n",
      "Epoch 169/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.7135e-05 - val_loss: 9.3725e-05\n",
      "Epoch 00169: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xae06a4ebe0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
