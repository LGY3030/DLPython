{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN(Long Short-Term Memory, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7cb100251672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "def readTrain():\n",
    "  train = pd.read_csv(\"S&P 500.csv\")\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment Features\n",
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "  train[\"date\"] = train[\"Date\"].dt.day\n",
    "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(train):\n",
    "  train = train.drop([\"Date\"], axis=1)\n",
    "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "  print(np.mean(train))\n",
    "  return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Training Data\n",
    "# X_train: 利用前30天的Open, High, Low, Close, Adj Close, Volume, month, year, date, day作為Features\n",
    "# Y_train: 利用未來5天的Adj Close作為Features\n",
    "# 須將資料做位移的展開作為Training Data\n",
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料亂序\n",
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將Training Data取一部份當作Validation Data\n",
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將輸出合併\n",
    "\n",
    "\n",
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型建置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一對一模型\n",
    "def buildOneToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 205us/step - loss: 0.0719 - val_loss: 0.0530\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0423 - val_loss: 0.0288\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 0.0202 - val_loss: 0.0113\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0014 - val_loss: 6.0240e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2767e-04 - val_loss: 3.1380e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8844e-04 - val_loss: 2.4050e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3580e-04 - val_loss: 2.0348e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.0651e-04 - val_loss: 1.8349e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.8863e-04 - val_loss: 1.7094e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.7582e-04 - val_loss: 1.6128e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.6582e-04 - val_loss: 1.5327e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.5672e-04 - val_loss: 1.4667e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.4863e-04 - val_loss: 1.3958e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.4088e-04 - val_loss: 1.3331e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.3393e-04 - val_loss: 1.2654e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.2674e-04 - val_loss: 1.2042e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.2072e-04 - val_loss: 1.1457e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 1.1417e-04 - val_loss: 1.0928e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.0796e-04 - val_loss: 1.0375e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.0231e-04 - val_loss: 9.8833e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 9.7084e-05 - val_loss: 9.4524e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 9.2196e-05 - val_loss: 9.0118e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 8.7479e-05 - val_loss: 8.5664e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 8.3385e-05 - val_loss: 8.1789e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 7.9294e-05 - val_loss: 7.8304e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 7.5514e-05 - val_loss: 7.5033e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 7.2270e-05 - val_loss: 7.1970e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 6.9077e-05 - val_loss: 6.9219e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 6.6406e-05 - val_loss: 6.6693e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 6.3536e-05 - val_loss: 6.4398e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 6.1279e-05 - val_loss: 6.2538e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.9183e-05 - val_loss: 6.0541e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.7391e-05 - val_loss: 5.8920e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.5574e-05 - val_loss: 5.7185e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 5.4027e-05 - val_loss: 5.5901e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.2544e-05 - val_loss: 5.4737e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 5.1513e-05 - val_loss: 5.3594e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 5.0367e-05 - val_loss: 5.2757e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.9313e-05 - val_loss: 5.2358e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.8687e-05 - val_loss: 5.1362e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 4.7711e-05 - val_loss: 5.0409e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 4.6875e-05 - val_loss: 5.0054e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.6368e-05 - val_loss: 4.9346e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.5640e-05 - val_loss: 4.8403e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.5074e-05 - val_loss: 4.7741e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.4609e-05 - val_loss: 4.7314e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.4171e-05 - val_loss: 4.6904e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.3744e-05 - val_loss: 4.6251e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.3203e-05 - val_loss: 4.6229e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2951e-05 - val_loss: 4.5717e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.2390e-05 - val_loss: 4.5100e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1975e-05 - val_loss: 4.4401e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 4.1649e-05 - val_loss: 4.4313e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.1305e-05 - val_loss: 4.3978e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.0896e-05 - val_loss: 4.3543e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.0730e-05 - val_loss: 4.3199e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.0266e-05 - val_loss: 4.2951e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.9974e-05 - val_loss: 4.2032e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.9700e-05 - val_loss: 4.1919e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 3.9271e-05 - val_loss: 4.1453e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.9273e-05 - val_loss: 4.1474e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.8566e-05 - val_loss: 4.0671e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.8517e-05 - val_loss: 4.0577e-05\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.7942e-05 - val_loss: 4.0275e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.7857e-05 - val_loss: 3.9670e-05\n",
      "Epoch 67/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.7489e-05 - val_loss: 3.9323e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.7128e-05 - val_loss: 3.9514e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.6947e-05 - val_loss: 3.8459e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.6622e-05 - val_loss: 3.8001e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.6201e-05 - val_loss: 3.8839e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.6176e-05 - val_loss: 3.7540e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.5740e-05 - val_loss: 3.7264e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.5136e-05 - val_loss: 3.7081e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.4976e-05 - val_loss: 3.6376e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.5173e-05 - val_loss: 3.6526e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.4430e-05 - val_loss: 3.6529e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.3989e-05 - val_loss: 3.5512e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.3820e-05 - val_loss: 3.5187e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.3457e-05 - val_loss: 3.4777e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.3416e-05 - val_loss: 3.5247e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.2945e-05 - val_loss: 3.4269e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2694e-05 - val_loss: 3.4436e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2698e-05 - val_loss: 3.3371e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2106e-05 - val_loss: 3.3818e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.2023e-05 - val_loss: 3.2766e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.1789e-05 - val_loss: 3.2887e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.1578e-05 - val_loss: 3.2666e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.1401e-05 - val_loss: 3.1756e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.0889e-05 - val_loss: 3.2199e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 3.0988e-05 - val_loss: 3.1900e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.0338e-05 - val_loss: 3.1395e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 3.0221e-05 - val_loss: 3.0806e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 3.0592e-05 - val_loss: 3.1241e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9713e-05 - val_loss: 3.0946e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9426e-05 - val_loss: 2.9963e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9350e-05 - val_loss: 2.9789e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.8981e-05 - val_loss: 2.9430e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.9120e-05 - val_loss: 2.9383e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.8667e-05 - val_loss: 2.9285e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8073e-05 - val_loss: 2.8919e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.8025e-05 - val_loss: 2.8736e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.8047e-05 - val_loss: 2.8638e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7739e-05 - val_loss: 2.7909e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.7663e-05 - val_loss: 2.7854e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7464e-05 - val_loss: 2.7557e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7331e-05 - val_loss: 2.8118e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7797e-05 - val_loss: 2.6995e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7097e-05 - val_loss: 2.7469e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6718e-05 - val_loss: 2.7295e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6488e-05 - val_loss: 2.6500e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6662e-05 - val_loss: 2.7889e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.7179e-05 - val_loss: 2.6123e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.6214e-05 - val_loss: 2.6907e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.6367e-05 - val_loss: 2.6187e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5693e-05 - val_loss: 2.5685e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.5629e-05 - val_loss: 2.5564e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.5455e-05 - val_loss: 2.5133e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.5392e-05 - val_loss: 2.5896e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.5660e-05 - val_loss: 2.6084e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5628e-05 - val_loss: 2.5788e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5735e-05 - val_loss: 2.5945e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5419e-05 - val_loss: 2.4690e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4902e-05 - val_loss: 2.5331e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5684e-05 - val_loss: 2.4478e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5011e-05 - val_loss: 2.5408e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4670e-05 - val_loss: 2.4816e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.5275e-05 - val_loss: 2.4171e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4574e-05 - val_loss: 2.4361e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4802e-05 - val_loss: 2.5855e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4683e-05 - val_loss: 2.4253e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4532e-05 - val_loss: 2.3940e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4597e-05 - val_loss: 2.3766e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4444e-05 - val_loss: 2.3769e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4340e-05 - val_loss: 2.4012e-05\n",
      "Epoch 136/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4286e-05 - val_loss: 2.3790e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4663e-05 - val_loss: 2.3289e-05\n",
      "Epoch 138/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4514e-05 - val_loss: 2.3673e-05\n",
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4260e-05 - val_loss: 2.3699e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4299e-05 - val_loss: 2.3772e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4421e-05 - val_loss: 2.3600e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4324e-05 - val_loss: 2.3893e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3924e-05 - val_loss: 2.3471e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3819e-05 - val_loss: 2.2929e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3921e-05 - val_loss: 2.2980e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3877e-05 - val_loss: 2.3387e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4160e-05 - val_loss: 2.2622e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4274e-05 - val_loss: 2.3127e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3714e-05 - val_loss: 2.3893e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3879e-05 - val_loss: 2.3308e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3566e-05 - val_loss: 2.2978e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3986e-05 - val_loss: 2.2632e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3501e-05 - val_loss: 2.2879e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3486e-05 - val_loss: 2.3202e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3945e-05 - val_loss: 2.8493e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.4438e-05 - val_loss: 2.2932e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3864e-05 - val_loss: 2.3802e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 7us/step - loss: 2.4057e-05 - val_loss: 2.3951e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3571e-05 - val_loss: 2.2337e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3590e-05 - val_loss: 2.3682e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 8us/step - loss: 2.3892e-05 - val_loss: 2.4794e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3966e-05 - val_loss: 2.2753e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3372e-05 - val_loss: 2.2673e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3776e-05 - val_loss: 2.3631e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3672e-05 - val_loss: 2.3115e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3294e-05 - val_loss: 2.2192e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3700e-05 - val_loss: 2.1963e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3368e-05 - val_loss: 2.3908e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3899e-05 - val_loss: 2.6152e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4292e-05 - val_loss: 2.3690e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3887e-05 - val_loss: 2.2354e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3087e-05 - val_loss: 2.2150e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4385e-05 - val_loss: 2.2959e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3557e-05 - val_loss: 2.2440e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3439e-05 - val_loss: 2.6566e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4079e-05 - val_loss: 2.2641e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3747e-05 - val_loss: 2.2353e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3120e-05 - val_loss: 2.3348e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3884e-05 - val_loss: 2.2600e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3339e-05 - val_loss: 2.2786e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3466e-05 - val_loss: 2.3026e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3918e-05 - val_loss: 2.2012e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x83b904e748>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將過去的天數pastDay設為1，預測的天數futureDay也設為1\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多對一模型\n",
    "def buildManyToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 1s 218us/step - loss: 0.1257 - val_loss: 0.0486\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 0.0196 - val_loss: 0.0065\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 0.0011 - val_loss: 8.5541e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 6.9563e-04 - val_loss: 5.9016e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 5.0966e-04 - val_loss: 4.5814e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.0331e-04 - val_loss: 3.7347e-04\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.3252e-04 - val_loss: 3.1392e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 2.8087e-04 - val_loss: 2.7167e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 2.4373e-04 - val_loss: 2.3463e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 2.1399e-04 - val_loss: 2.0905e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 1.9134e-04 - val_loss: 1.8760e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 1.7165e-04 - val_loss: 1.7363e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 1.5635e-04 - val_loss: 1.5760e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 1.4211e-04 - val_loss: 1.4312e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 1.3008e-04 - val_loss: 1.3225e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 1.2050e-04 - val_loss: 1.2283e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 1.1186e-04 - val_loss: 1.1466e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 1.0492e-04 - val_loss: 1.0727e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 9.8699e-05 - val_loss: 1.0224e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 9.2407e-05 - val_loss: 9.6723e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 8.6877e-05 - val_loss: 9.1194e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 8.2429e-05 - val_loss: 8.6453e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 7.8877e-05 - val_loss: 8.6470e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 7.5354e-05 - val_loss: 7.9294e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 7.1492e-05 - val_loss: 7.6139e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 6.9161e-05 - val_loss: 7.3568e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 6.6439e-05 - val_loss: 7.1530e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 6.5530e-05 - val_loss: 7.0481e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 6.3161e-05 - val_loss: 6.7205e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 6.2070e-05 - val_loss: 6.6103e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 5.9796e-05 - val_loss: 6.4807e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 5.8372e-05 - val_loss: 6.2563e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 5.7146e-05 - val_loss: 6.2181e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 5.6851e-05 - val_loss: 6.1653e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 5.5088e-05 - val_loss: 6.0948e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 5.4625e-05 - val_loss: 5.9481e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 5.3401e-05 - val_loss: 5.7888e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 5.2820e-05 - val_loss: 5.9148e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 5.2940e-05 - val_loss: 5.6613e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 5.1197e-05 - val_loss: 5.7002e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.1317e-05 - val_loss: 5.5511e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 5.0877e-05 - val_loss: 5.5079e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 5.0373e-05 - val_loss: 5.4157e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.9761e-05 - val_loss: 5.4984e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.9515e-05 - val_loss: 5.3918e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.8849e-05 - val_loss: 5.2889e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.7399e-05 - val_loss: 5.2071e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.6992e-05 - val_loss: 5.1344e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.6948e-05 - val_loss: 5.1169e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.6299e-05 - val_loss: 5.2099e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 4.5546e-05 - val_loss: 5.0232e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.5489e-05 - val_loss: 5.0188e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.5096e-05 - val_loss: 5.0030e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.5081e-05 - val_loss: 5.0907e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.4317e-05 - val_loss: 4.9828e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.4394e-05 - val_loss: 5.0036e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 4.4538e-05 - val_loss: 4.8203e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.3608e-05 - val_loss: 4.6873e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.2883e-05 - val_loss: 4.7087e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.2450e-05 - val_loss: 4.6999e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 4.2261e-05 - val_loss: 4.8767e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 4.2069e-05 - val_loss: 4.9070e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.2578e-05 - val_loss: 4.7287e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.1293e-05 - val_loss: 4.5583e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.1725e-05 - val_loss: 4.8249e-05\n",
      "Epoch 67/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.0835e-05 - val_loss: 4.4943e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.1653e-05 - val_loss: 4.5527e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.0118e-05 - val_loss: 4.4816e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.0065e-05 - val_loss: 4.3973e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.9060e-05 - val_loss: 4.3693e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.9249e-05 - val_loss: 4.5204e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.9636e-05 - val_loss: 4.3169e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.9580e-05 - val_loss: 4.4748e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.8790e-05 - val_loss: 4.5204e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.8750e-05 - val_loss: 4.2960e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.9070e-05 - val_loss: 4.5233e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.7468e-05 - val_loss: 4.1849e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.7920e-05 - val_loss: 4.2070e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.7898e-05 - val_loss: 4.1789e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.7842e-05 - val_loss: 4.1753e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.7144e-05 - val_loss: 4.2481e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.7213e-05 - val_loss: 4.1187e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.6865e-05 - val_loss: 4.8201e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.7383e-05 - val_loss: 4.6219e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.6656e-05 - val_loss: 4.0093e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.7181e-05 - val_loss: 4.0670e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.5632e-05 - val_loss: 4.0721e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.5398e-05 - val_loss: 3.9149e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.5345e-05 - val_loss: 4.0081e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.6644e-05 - val_loss: 3.9093e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.5891e-05 - val_loss: 4.0962e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.5291e-05 - val_loss: 4.0077e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.4835e-05 - val_loss: 3.7985e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.4564e-05 - val_loss: 3.8075e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.4413e-05 - val_loss: 3.8278e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.4749e-05 - val_loss: 4.0004e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.4230e-05 - val_loss: 3.9915e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.4556e-05 - val_loss: 3.8909e-05\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.3853e-05 - val_loss: 3.7328e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.3075e-05 - val_loss: 3.6711e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.3446e-05 - val_loss: 3.7022e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.2846e-05 - val_loss: 3.6842e-05\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.3474e-05 - val_loss: 4.3125e-05\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.4536e-05 - val_loss: 3.6747e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.2356e-05 - val_loss: 3.5999e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.4703e-05 - val_loss: 3.8857e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.2215e-05 - val_loss: 3.8842e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.2905e-05 - val_loss: 3.6034e-05\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.2003e-05 - val_loss: 3.5235e-05\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 3.2364e-05 - val_loss: 4.1710e-05\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 3.1985e-05 - val_loss: 3.4097e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.2161e-05 - val_loss: 3.4641e-05\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.3143e-05 - val_loss: 3.8471e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.2947e-05 - val_loss: 3.4255e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.2527e-05 - val_loss: 3.6539e-05\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.1903e-05 - val_loss: 4.0023e-05\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.2163e-05 - val_loss: 3.5611e-05\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.1046e-05 - val_loss: 3.6700e-05\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.0838e-05 - val_loss: 3.5122e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.2392e-05 - val_loss: 3.8865e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.0469e-05 - val_loss: 3.4984e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.1185e-05 - val_loss: 3.2985e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.1100e-05 - val_loss: 3.4736e-05\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.1006e-05 - val_loss: 3.3676e-05\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.2167e-05 - val_loss: 3.5044e-05\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 3.0695e-05 - val_loss: 3.8043e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.0024e-05 - val_loss: 3.8415e-05\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.0103e-05 - val_loss: 3.3047e-05\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 2.9705e-05 - val_loss: 3.3984e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.9630e-05 - val_loss: 3.2473e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 2.9235e-05 - val_loss: 3.2381e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.0663e-05 - val_loss: 3.5246e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.9446e-05 - val_loss: 3.1536e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.8545e-05 - val_loss: 3.1694e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.0342e-05 - val_loss: 3.4675e-05\n",
      "Epoch 137/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.9768e-05 - val_loss: 3.2927e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.9556e-05 - val_loss: 3.1998e-05\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8899e-05 - val_loss: 3.6291e-05\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.8457e-05 - val_loss: 3.4714e-05\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.8341e-05 - val_loss: 3.0928e-05\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.9019e-05 - val_loss: 3.2611e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.8555e-05 - val_loss: 3.0727e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.8504e-05 - val_loss: 3.4873e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.8539e-05 - val_loss: 3.0734e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8327e-05 - val_loss: 3.5098e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.0040e-05 - val_loss: 5.0985e-05\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.9128e-05 - val_loss: 3.0125e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.8047e-05 - val_loss: 3.1162e-05\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.0891e-05 - val_loss: 3.1357e-05\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 2.8077e-05 - val_loss: 3.3819e-05\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.7821e-05 - val_loss: 3.1053e-05\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.6858e-05 - val_loss: 2.9618e-05\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.9281e-05 - val_loss: 3.3808e-05\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.9187e-05 - val_loss: 3.0007e-05\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.0964e-05 - val_loss: 3.0559e-05\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.7196e-05 - val_loss: 3.0977e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.9065e-05 - val_loss: 3.5643e-05\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.7954e-05 - val_loss: 3.0684e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.6584e-05 - val_loss: 3.1770e-05\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.6406e-05 - val_loss: 2.8835e-05\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 2.7200e-05 - val_loss: 3.0105e-05\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 2.6983e-05 - val_loss: 3.2208e-05\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.7194e-05 - val_loss: 3.0348e-05\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.7255e-05 - val_loss: 2.8841e-05\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8474e-05 - val_loss: 3.0409e-05\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.6425e-05 - val_loss: 3.1357e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 2.8816e-05 - val_loss: 3.4415e-05\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.8436e-05 - val_loss: 3.2734e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.7596e-05 - val_loss: 2.9426e-05\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 2.7002e-05 - val_loss: 3.1802e-05\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x83bf30bdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要設定的有pastDay=30、future=1 ，且Y_train 的維度需為二維\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一對多模型\n",
    "def buildOneToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(Dense(1))\n",
    "  model.add(RepeatVector(5))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(1, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5706 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5706/5706 [==============================] - 1s 138us/step - loss: 0.0717 - val_loss: 0.0550\n",
      "Epoch 2/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 0.0421 - val_loss: 0.0293\n",
      "Epoch 3/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 0.0201 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 0.0064 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 0.0014 - val_loss: 6.1696e-04\n",
      "Epoch 6/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 4.6643e-04 - val_loss: 3.6124e-04\n",
      "Epoch 7/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 3.3111e-04 - val_loss: 2.9115e-04\n",
      "Epoch 8/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.7876e-04 - val_loss: 2.5423e-04\n",
      "Epoch 9/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.4911e-04 - val_loss: 2.3421e-04\n",
      "Epoch 10/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.3095e-04 - val_loss: 2.1910e-04\n",
      "Epoch 11/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.1792e-04 - val_loss: 2.0790e-04\n",
      "Epoch 12/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 2.0690e-04 - val_loss: 1.9834e-04\n",
      "Epoch 13/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.9754e-04 - val_loss: 1.9025e-04\n",
      "Epoch 14/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.8899e-04 - val_loss: 1.8226e-04\n",
      "Epoch 15/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.8078e-04 - val_loss: 1.7552e-04\n",
      "Epoch 16/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.7351e-04 - val_loss: 1.6834e-04\n",
      "Epoch 17/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.6642e-04 - val_loss: 1.6215e-04\n",
      "Epoch 18/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.5911e-04 - val_loss: 1.5533e-04\n",
      "Epoch 19/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.5246e-04 - val_loss: 1.4936e-04\n",
      "Epoch 20/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.4637e-04 - val_loss: 1.4363e-04\n",
      "Epoch 21/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.4045e-04 - val_loss: 1.3842e-04\n",
      "Epoch 22/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.3492e-04 - val_loss: 1.3216e-04\n",
      "Epoch 23/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.2967e-04 - val_loss: 1.2791e-04\n",
      "Epoch 24/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.2481e-04 - val_loss: 1.2264e-04\n",
      "Epoch 25/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.2059e-04 - val_loss: 1.1867e-04\n",
      "Epoch 26/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.1611e-04 - val_loss: 1.1512e-04\n",
      "Epoch 27/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.1236e-04 - val_loss: 1.1116e-04\n",
      "Epoch 28/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.0868e-04 - val_loss: 1.0821e-04\n",
      "Epoch 29/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.0547e-04 - val_loss: 1.0475e-04\n",
      "Epoch 30/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.0232e-04 - val_loss: 1.0184e-04\n",
      "Epoch 31/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.9603e-05 - val_loss: 9.9640e-05\n",
      "Epoch 32/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 9.7489e-05 - val_loss: 9.7610e-05\n",
      "Epoch 33/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.5114e-05 - val_loss: 9.6272e-05\n",
      "Epoch 34/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.3212e-05 - val_loss: 9.3787e-05\n",
      "Epoch 35/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 9.1413e-05 - val_loss: 9.1636e-05\n",
      "Epoch 36/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.9711e-05 - val_loss: 8.9538e-05\n",
      "Epoch 37/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.8300e-05 - val_loss: 8.8376e-05\n",
      "Epoch 38/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.6818e-05 - val_loss: 8.7311e-05\n",
      "Epoch 39/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.6003e-05 - val_loss: 8.5904e-05\n",
      "Epoch 40/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.4818e-05 - val_loss: 8.5362e-05\n",
      "Epoch 41/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.3799e-05 - val_loss: 8.4003e-05\n",
      "Epoch 42/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.2925e-05 - val_loss: 8.3549e-05\n",
      "Epoch 43/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.2328e-05 - val_loss: 8.2433e-05\n",
      "Epoch 44/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.1523e-05 - val_loss: 8.1590e-05\n",
      "Epoch 45/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.1091e-05 - val_loss: 8.0890e-05\n",
      "Epoch 46/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.0500e-05 - val_loss: 8.1579e-05\n",
      "Epoch 47/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.9713e-05 - val_loss: 7.9560e-05\n",
      "Epoch 48/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.9351e-05 - val_loss: 7.9910e-05\n",
      "Epoch 49/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.8598e-05 - val_loss: 7.9220e-05\n",
      "Epoch 50/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.8583e-05 - val_loss: 7.8281e-05\n",
      "Epoch 51/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.7874e-05 - val_loss: 7.7594e-05\n",
      "Epoch 52/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.7871e-05 - val_loss: 7.7047e-05\n",
      "Epoch 53/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.7252e-05 - val_loss: 7.6382e-05\n",
      "Epoch 54/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.6990e-05 - val_loss: 7.6437e-05\n",
      "Epoch 55/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6404e-05 - val_loss: 7.6135e-05\n",
      "Epoch 56/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.6459e-05 - val_loss: 7.5847e-05\n",
      "Epoch 57/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.5930e-05 - val_loss: 7.5024e-05\n",
      "Epoch 58/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 7.5228e-05 - val_loss: 7.4308e-05\n",
      "Epoch 59/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 7.4908e-05 - val_loss: 7.3976e-05\n",
      "Epoch 60/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.4586e-05 - val_loss: 7.4159e-05\n",
      "Epoch 61/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.3970e-05 - val_loss: 7.4285e-05\n",
      "Epoch 62/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.3966e-05 - val_loss: 7.3345e-05\n",
      "Epoch 63/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.3585e-05 - val_loss: 7.3826e-05\n",
      "Epoch 64/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 7.3206e-05 - val_loss: 7.2112e-05\n",
      "Epoch 65/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.2912e-05 - val_loss: 7.2223e-05\n",
      "Epoch 66/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.2695e-05 - val_loss: 7.1587e-05\n",
      "Epoch 67/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.2096e-05 - val_loss: 7.2373e-05\n",
      "Epoch 68/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.1903e-05 - val_loss: 7.0980e-05\n",
      "Epoch 69/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.1651e-05 - val_loss: 7.0414e-05\n",
      "Epoch 70/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 7.1333e-05 - val_loss: 6.9435e-05\n",
      "Epoch 71/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.0921e-05 - val_loss: 6.9512e-05\n",
      "Epoch 72/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.0287e-05 - val_loss: 7.0551e-05\n",
      "Epoch 73/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.0699e-05 - val_loss: 6.9041e-05\n",
      "Epoch 74/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.0311e-05 - val_loss: 6.9648e-05\n",
      "Epoch 75/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.9706e-05 - val_loss: 6.8232e-05\n",
      "Epoch 76/1000\n",
      "5706/5706 [==============================] - 0s 7us/step - loss: 6.9065e-05 - val_loss: 6.8050e-05\n",
      "Epoch 77/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.9096e-05 - val_loss: 6.7371e-05\n",
      "Epoch 78/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.8433e-05 - val_loss: 6.7317e-05\n",
      "Epoch 79/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8029e-05 - val_loss: 6.6997e-05\n",
      "Epoch 80/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8180e-05 - val_loss: 6.6673e-05\n",
      "Epoch 81/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.8445e-05 - val_loss: 6.6309e-05\n",
      "Epoch 82/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.7553e-05 - val_loss: 6.6730e-05\n",
      "Epoch 83/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.7527e-05 - val_loss: 6.5856e-05\n",
      "Epoch 84/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.7321e-05 - val_loss: 6.4789e-05\n",
      "Epoch 85/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.6753e-05 - val_loss: 6.4412e-05\n",
      "Epoch 86/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.6560e-05 - val_loss: 6.6993e-05\n",
      "Epoch 87/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.6060e-05 - val_loss: 6.4594e-05\n",
      "Epoch 88/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.5779e-05 - val_loss: 6.3864e-05\n",
      "Epoch 89/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 6.5408e-05 - val_loss: 6.3526e-05\n",
      "Epoch 90/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5036e-05 - val_loss: 6.2768e-05\n",
      "Epoch 91/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.5111e-05 - val_loss: 6.2786e-05\n",
      "Epoch 92/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.4285e-05 - val_loss: 6.2115e-05\n",
      "Epoch 93/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.4567e-05 - val_loss: 6.4625e-05\n",
      "Epoch 94/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4570e-05 - val_loss: 6.2627e-05\n",
      "Epoch 95/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3936e-05 - val_loss: 6.2148e-05\n",
      "Epoch 96/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.3547e-05 - val_loss: 6.2465e-05\n",
      "Epoch 97/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.3906e-05 - val_loss: 6.2894e-05\n",
      "Epoch 98/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 6.3286e-05 - val_loss: 6.1249e-05\n",
      "Epoch 99/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2930e-05 - val_loss: 6.2254e-05\n",
      "Epoch 100/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2762e-05 - val_loss: 6.1404e-05\n",
      "Epoch 101/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2607e-05 - val_loss: 6.0130e-05\n",
      "Epoch 102/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2361e-05 - val_loss: 6.1579e-05\n",
      "Epoch 103/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2580e-05 - val_loss: 5.9460e-05\n",
      "Epoch 104/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2223e-05 - val_loss: 5.9579e-05\n",
      "Epoch 105/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.2058e-05 - val_loss: 6.0024e-05\n",
      "Epoch 106/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.1259e-05 - val_loss: 5.8888e-05\n",
      "Epoch 107/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.1220e-05 - val_loss: 5.9586e-05\n",
      "Epoch 108/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.1356e-05 - val_loss: 5.8381e-05\n",
      "Epoch 109/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.0669e-05 - val_loss: 5.8369e-05\n",
      "Epoch 110/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 6.0983e-05 - val_loss: 5.9496e-05\n",
      "Epoch 111/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0251e-05 - val_loss: 5.8323e-05\n",
      "Epoch 112/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0228e-05 - val_loss: 5.8940e-05\n",
      "Epoch 113/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0122e-05 - val_loss: 5.8296e-05\n",
      "Epoch 114/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9841e-05 - val_loss: 5.7628e-05\n",
      "Epoch 115/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0475e-05 - val_loss: 5.7205e-05\n",
      "Epoch 116/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9662e-05 - val_loss: 5.6807e-05\n",
      "Epoch 117/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.9142e-05 - val_loss: 5.7702e-05\n",
      "Epoch 118/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9151e-05 - val_loss: 5.6839e-05\n",
      "Epoch 119/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9213e-05 - val_loss: 5.6826e-05\n",
      "Epoch 120/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9197e-05 - val_loss: 5.8850e-05\n",
      "Epoch 121/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9370e-05 - val_loss: 5.7437e-05\n",
      "Epoch 122/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8790e-05 - val_loss: 5.7921e-05\n",
      "Epoch 123/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8555e-05 - val_loss: 5.6520e-05\n",
      "Epoch 124/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.8631e-05 - val_loss: 5.6936e-05\n",
      "Epoch 125/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.8034e-05 - val_loss: 5.6077e-05\n",
      "Epoch 126/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.8844e-05 - val_loss: 5.6825e-05\n",
      "Epoch 127/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.8708e-05 - val_loss: 5.5857e-05\n",
      "Epoch 128/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.8039e-05 - val_loss: 5.7026e-05\n",
      "Epoch 129/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.8106e-05 - val_loss: 5.5079e-05\n",
      "Epoch 130/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8304e-05 - val_loss: 5.7595e-05\n",
      "Epoch 131/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.8339e-05 - val_loss: 5.5725e-05\n",
      "Epoch 132/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8168e-05 - val_loss: 5.5366e-05\n",
      "Epoch 133/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7957e-05 - val_loss: 5.5750e-05\n",
      "Epoch 134/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8086e-05 - val_loss: 5.6603e-05\n",
      "Epoch 135/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.9212e-05 - val_loss: 5.6237e-05\n",
      "Epoch 136/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8520e-05 - val_loss: 5.4632e-05\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7311e-05 - val_loss: 5.4996e-05\n",
      "Epoch 138/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7560e-05 - val_loss: 5.7050e-05\n",
      "Epoch 139/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.7824e-05 - val_loss: 5.5398e-05\n",
      "Epoch 140/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7564e-05 - val_loss: 5.5922e-05\n",
      "Epoch 141/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7269e-05 - val_loss: 5.6631e-05\n",
      "Epoch 142/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.7574e-05 - val_loss: 5.5467e-05\n",
      "Epoch 143/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7079e-05 - val_loss: 5.6559e-05\n",
      "Epoch 144/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.7266e-05 - val_loss: 5.5439e-05\n",
      "Epoch 145/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7253e-05 - val_loss: 5.6193e-05\n",
      "Epoch 146/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7053e-05 - val_loss: 5.4792e-05\n",
      "Epoch 147/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7562e-05 - val_loss: 5.9133e-05\n",
      "Epoch 148/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.7446e-05 - val_loss: 5.5042e-05\n",
      "Epoch 149/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7210e-05 - val_loss: 5.6840e-05\n",
      "Epoch 150/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7533e-05 - val_loss: 5.8170e-05\n",
      "Epoch 151/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7106e-05 - val_loss: 5.6046e-05\n",
      "Epoch 152/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7383e-05 - val_loss: 5.7065e-05\n",
      "Epoch 153/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7465e-05 - val_loss: 5.5528e-05\n",
      "Epoch 154/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6786e-05 - val_loss: 5.4161e-05\n",
      "Epoch 155/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7150e-05 - val_loss: 5.4620e-05\n",
      "Epoch 156/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7364e-05 - val_loss: 5.5647e-05\n",
      "Epoch 157/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6972e-05 - val_loss: 5.4847e-05\n",
      "Epoch 158/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6677e-05 - val_loss: 5.8009e-05\n",
      "Epoch 159/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.7739e-05 - val_loss: 5.4829e-05\n",
      "Epoch 160/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6444e-05 - val_loss: 5.6123e-05\n",
      "Epoch 161/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6678e-05 - val_loss: 5.5081e-05\n",
      "Epoch 162/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6490e-05 - val_loss: 5.4704e-05\n",
      "Epoch 163/1000\n",
      "5706/5706 [==============================] - 0s 7us/step - loss: 5.6557e-05 - val_loss: 5.5005e-05\n",
      "Epoch 164/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6806e-05 - val_loss: 5.6075e-05\n",
      "Epoch 165/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6757e-05 - val_loss: 5.3982e-05\n",
      "Epoch 166/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6285e-05 - val_loss: 5.4432e-05\n",
      "Epoch 167/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6224e-05 - val_loss: 5.4410e-05\n",
      "Epoch 168/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6384e-05 - val_loss: 5.6460e-05\n",
      "Epoch 169/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6836e-05 - val_loss: 5.4093e-05\n",
      "Epoch 170/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6466e-05 - val_loss: 5.5009e-05\n",
      "Epoch 171/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.6812e-05 - val_loss: 5.4799e-05\n",
      "Epoch 172/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.6754e-05 - val_loss: 5.6230e-05\n",
      "Epoch 173/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7583e-05 - val_loss: 5.4292e-05\n",
      "Epoch 174/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6050e-05 - val_loss: 5.6540e-05\n",
      "Epoch 175/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6160e-05 - val_loss: 5.4088e-05\n",
      "Epoch 176/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6483e-05 - val_loss: 5.5878e-05\n",
      "Epoch 177/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6414e-05 - val_loss: 5.4593e-05\n",
      "Epoch 178/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6385e-05 - val_loss: 5.6367e-05\n",
      "Epoch 179/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.5927e-05 - val_loss: 5.4399e-05\n",
      "Epoch 180/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.7255e-05 - val_loss: 5.5392e-05\n",
      "Epoch 181/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6140e-05 - val_loss: 5.4166e-05\n",
      "Epoch 182/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6250e-05 - val_loss: 5.4309e-05\n",
      "Epoch 183/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6678e-05 - val_loss: 5.6943e-05\n",
      "Epoch 184/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.8075e-05 - val_loss: 5.5199e-05\n",
      "Epoch 185/1000\n",
      "5706/5706 [==============================] - 0s 8us/step - loss: 5.6510e-05 - val_loss: 5.4962e-05\n",
      "Epoch 186/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6536e-05 - val_loss: 5.4866e-05\n",
      "Epoch 187/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6398e-05 - val_loss: 5.4499e-05\n",
      "Epoch 188/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 5.6489e-05 - val_loss: 5.5534e-05\n",
      "Epoch 189/1000\n",
      "5706/5706 [==============================] - 0s 9us/step - loss: 5.6504e-05 - val_loss: 5.4521e-05\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x83bfcdfe48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將pastDay 設為1, futureDay 設為5\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多對多模型 (輸入與輸出相同長度)\n",
    "def buildManyToManyModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
    "  # output shape: (5, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(5, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5703 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "5703/5703 [==============================] - 1s 167us/step - loss: 0.0996 - val_loss: 0.0551\n",
      "Epoch 2/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 0.0359 - val_loss: 0.0169\n",
      "Epoch 3/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 0.0125 - val_loss: 0.0087\n",
      "Epoch 4/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 5/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 6/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 0.0054 - val_loss: 0.0046\n",
      "Epoch 7/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 8/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 9/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 10/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 11/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 12/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 13/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 0.0011 - val_loss: 9.6671e-04\n",
      "Epoch 14/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8256e-04 - val_loss: 7.9957e-04\n",
      "Epoch 15/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 7.3523e-04 - val_loss: 6.7130e-04\n",
      "Epoch 16/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 6.2131e-04 - val_loss: 5.7293e-04\n",
      "Epoch 17/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 5.3241e-04 - val_loss: 4.9565e-04\n",
      "Epoch 18/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 4.6177e-04 - val_loss: 4.3462e-04\n",
      "Epoch 19/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 4.0551e-04 - val_loss: 3.8588e-04\n",
      "Epoch 20/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 3.6092e-04 - val_loss: 3.4674e-04\n",
      "Epoch 21/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 3.2526e-04 - val_loss: 3.1382e-04\n",
      "Epoch 22/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 2.9612e-04 - val_loss: 2.8882e-04\n",
      "Epoch 23/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 2.7243e-04 - val_loss: 2.6686e-04\n",
      "Epoch 24/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 2.5264e-04 - val_loss: 2.4770e-04\n",
      "Epoch 25/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 2.3582e-04 - val_loss: 2.3210e-04\n",
      "Epoch 26/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 2.2203e-04 - val_loss: 2.1850e-04\n",
      "Epoch 27/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 2.0965e-04 - val_loss: 2.0670e-04\n",
      "Epoch 28/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.9897e-04 - val_loss: 1.9631e-04\n",
      "Epoch 29/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 1.8940e-04 - val_loss: 1.8735e-04\n",
      "Epoch 30/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.8134e-04 - val_loss: 1.7952e-04\n",
      "Epoch 31/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.7393e-04 - val_loss: 1.7213e-04\n",
      "Epoch 32/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.6735e-04 - val_loss: 1.6663e-04\n",
      "Epoch 33/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 1.6172e-04 - val_loss: 1.6026e-04\n",
      "Epoch 34/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 1.5609e-04 - val_loss: 1.5584e-04\n",
      "Epoch 35/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.5189e-04 - val_loss: 1.5096e-04\n",
      "Epoch 36/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.4740e-04 - val_loss: 1.4709e-04\n",
      "Epoch 37/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.4373e-04 - val_loss: 1.4334e-04\n",
      "Epoch 38/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.4047e-04 - val_loss: 1.4160e-04\n",
      "Epoch 39/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.3727e-04 - val_loss: 1.3698e-04\n",
      "Epoch 40/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.3409e-04 - val_loss: 1.3448e-04\n",
      "Epoch 41/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.3171e-04 - val_loss: 1.3211e-04\n",
      "Epoch 42/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.2922e-04 - val_loss: 1.2981e-04\n",
      "Epoch 43/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.2713e-04 - val_loss: 1.2865e-04\n",
      "Epoch 44/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.2528e-04 - val_loss: 1.2776e-04\n",
      "Epoch 45/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.2332e-04 - val_loss: 1.2453e-04\n",
      "Epoch 46/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.2166e-04 - val_loss: 1.2320e-04\n",
      "Epoch 47/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.2072e-04 - val_loss: 1.2133e-04\n",
      "Epoch 48/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1876e-04 - val_loss: 1.2032e-04\n",
      "Epoch 49/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.1757e-04 - val_loss: 1.1992e-04\n",
      "Epoch 50/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.1624e-04 - val_loss: 1.1864e-04\n",
      "Epoch 51/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1484e-04 - val_loss: 1.1804e-04\n",
      "Epoch 52/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1390e-04 - val_loss: 1.1682e-04\n",
      "Epoch 53/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1321e-04 - val_loss: 1.1519e-04\n",
      "Epoch 54/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 1.1174e-04 - val_loss: 1.1526e-04\n",
      "Epoch 55/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 1.1096e-04 - val_loss: 1.1426e-04\n",
      "Epoch 56/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.1019e-04 - val_loss: 1.1303e-04\n",
      "Epoch 57/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0936e-04 - val_loss: 1.1540e-04\n",
      "Epoch 58/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0838e-04 - val_loss: 1.1149e-04\n",
      "Epoch 59/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.0752e-04 - val_loss: 1.1050e-04\n",
      "Epoch 60/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 1.0810e-04 - val_loss: 1.1119e-04\n",
      "Epoch 61/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.0671e-04 - val_loss: 1.1077e-04\n",
      "Epoch 62/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0533e-04 - val_loss: 1.0988e-04\n",
      "Epoch 63/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.0476e-04 - val_loss: 1.1011e-04\n",
      "Epoch 64/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.0399e-04 - val_loss: 1.0804e-04\n",
      "Epoch 65/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.0349e-04 - val_loss: 1.0789e-04\n",
      "Epoch 66/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 1.0310e-04 - val_loss: 1.0717e-04\n",
      "Epoch 67/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 1.0245e-04 - val_loss: 1.0893e-04\n",
      "Epoch 68/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 1.0267e-04 - val_loss: 1.0648e-04\n",
      "Epoch 69/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 1.0090e-04 - val_loss: 1.0764e-04\n",
      "Epoch 70/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 1.0066e-04 - val_loss: 1.0639e-04\n",
      "Epoch 71/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 1.0002e-04 - val_loss: 1.0517e-04\n",
      "Epoch 72/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.9756e-05 - val_loss: 1.0402e-04\n",
      "Epoch 73/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.9294e-05 - val_loss: 1.0538e-04\n",
      "Epoch 74/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.8785e-05 - val_loss: 1.0456e-04\n",
      "Epoch 75/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.8198e-05 - val_loss: 1.0309e-04\n",
      "Epoch 76/1000\n",
      "5703/5703 [==============================] - 0s 18us/step - loss: 9.7805e-05 - val_loss: 1.0376e-04\n",
      "Epoch 77/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.7191e-05 - val_loss: 1.0311e-04\n",
      "Epoch 78/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.6672e-05 - val_loss: 1.0196e-04\n",
      "Epoch 79/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.6845e-05 - val_loss: 1.0501e-04\n",
      "Epoch 80/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.6225e-05 - val_loss: 1.0170e-04\n",
      "Epoch 81/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.6136e-05 - val_loss: 1.0150e-04\n",
      "Epoch 82/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.5112e-05 - val_loss: 1.0144e-04\n",
      "Epoch 83/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.5251e-05 - val_loss: 1.0238e-04\n",
      "Epoch 84/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.5001e-05 - val_loss: 1.0127e-04\n",
      "Epoch 85/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.4863e-05 - val_loss: 1.0041e-04\n",
      "Epoch 86/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.4229e-05 - val_loss: 1.0062e-04\n",
      "Epoch 87/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.3872e-05 - val_loss: 1.0072e-04\n",
      "Epoch 88/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.3319e-05 - val_loss: 1.0028e-04\n",
      "Epoch 89/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.3001e-05 - val_loss: 9.9828e-05\n",
      "Epoch 90/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 9.3126e-05 - val_loss: 1.0334e-04\n",
      "Epoch 91/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.2562e-05 - val_loss: 9.9355e-05\n",
      "Epoch 92/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.2285e-05 - val_loss: 9.9361e-05\n",
      "Epoch 93/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.2570e-05 - val_loss: 1.0130e-04\n",
      "Epoch 94/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.2394e-05 - val_loss: 9.9400e-05\n",
      "Epoch 95/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 9.1352e-05 - val_loss: 9.8868e-05\n",
      "Epoch 96/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1899e-05 - val_loss: 9.8564e-05\n",
      "Epoch 97/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1918e-05 - val_loss: 9.8280e-05\n",
      "Epoch 98/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1313e-05 - val_loss: 9.8914e-05\n",
      "Epoch 99/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1279e-05 - val_loss: 9.8820e-05\n",
      "Epoch 100/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 9.0810e-05 - val_loss: 9.9799e-05\n",
      "Epoch 101/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0786e-05 - val_loss: 9.8866e-05\n",
      "Epoch 102/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1281e-05 - val_loss: 9.8348e-05\n",
      "Epoch 103/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0561e-05 - val_loss: 9.8694e-05\n",
      "Epoch 104/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0108e-05 - val_loss: 9.7707e-05\n",
      "Epoch 105/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 9.0222e-05 - val_loss: 9.9955e-05\n",
      "Epoch 106/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 9.0308e-05 - val_loss: 9.8674e-05\n",
      "Epoch 107/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0687e-05 - val_loss: 9.8234e-05\n",
      "Epoch 108/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0663e-05 - val_loss: 1.0028e-04\n",
      "Epoch 109/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0963e-05 - val_loss: 9.7514e-05\n",
      "Epoch 110/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9829e-05 - val_loss: 9.9446e-05\n",
      "Epoch 111/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.1416e-05 - val_loss: 9.8463e-05\n",
      "Epoch 112/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0983e-05 - val_loss: 9.8232e-05\n",
      "Epoch 113/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9791e-05 - val_loss: 9.6433e-05\n",
      "Epoch 114/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9777e-05 - val_loss: 9.7698e-05\n",
      "Epoch 115/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9961e-05 - val_loss: 1.0092e-04\n",
      "Epoch 116/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9040e-05 - val_loss: 9.7549e-05\n",
      "Epoch 117/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8796e-05 - val_loss: 9.6982e-05\n",
      "Epoch 118/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.9205e-05 - val_loss: 9.7616e-05\n",
      "Epoch 119/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0383e-05 - val_loss: 9.6747e-05\n",
      "Epoch 120/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 8.9618e-05 - val_loss: 9.6524e-05\n",
      "Epoch 121/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9419e-05 - val_loss: 9.6964e-05\n",
      "Epoch 122/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.9410e-05 - val_loss: 9.6067e-05\n",
      "Epoch 123/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8744e-05 - val_loss: 1.0011e-04\n",
      "Epoch 124/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0449e-05 - val_loss: 9.6982e-05\n",
      "Epoch 125/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9667e-05 - val_loss: 9.8312e-05\n",
      "Epoch 126/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8069e-05 - val_loss: 1.0077e-04\n",
      "Epoch 127/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.9088e-05 - val_loss: 9.6628e-05\n",
      "Epoch 128/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.9545e-05 - val_loss: 9.7826e-05\n",
      "Epoch 129/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 8.9678e-05 - val_loss: 9.5768e-05\n",
      "Epoch 130/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.8770e-05 - val_loss: 9.8513e-05\n",
      "Epoch 131/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.8562e-05 - val_loss: 9.9128e-05\n",
      "Epoch 132/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.8273e-05 - val_loss: 9.5751e-05\n",
      "Epoch 133/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8724e-05 - val_loss: 9.7461e-05\n",
      "Epoch 134/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8570e-05 - val_loss: 9.6749e-05\n",
      "Epoch 135/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.9214e-05 - val_loss: 9.6211e-05\n",
      "Epoch 136/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.7946e-05 - val_loss: 9.4816e-05\n",
      "Epoch 137/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.7945e-05 - val_loss: 9.7536e-05\n",
      "Epoch 138/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.7926e-05 - val_loss: 9.6357e-05\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8537e-05 - val_loss: 9.5499e-05\n",
      "Epoch 140/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.8913e-05 - val_loss: 9.7076e-05\n",
      "Epoch 141/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.7921e-05 - val_loss: 9.5884e-05\n",
      "Epoch 142/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.8644e-05 - val_loss: 9.9321e-05\n",
      "Epoch 143/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.7877e-05 - val_loss: 9.6408e-05\n",
      "Epoch 144/1000\n",
      "5703/5703 [==============================] - 0s 20us/step - loss: 8.9481e-05 - val_loss: 9.8832e-05\n",
      "Epoch 145/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 8.8740e-05 - val_loss: 9.5387e-05\n",
      "Epoch 146/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8477e-05 - val_loss: 9.5474e-05\n",
      "Epoch 147/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.9809e-05 - val_loss: 9.8688e-05\n",
      "Epoch 148/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8112e-05 - val_loss: 9.7019e-05\n",
      "Epoch 149/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 9.0637e-05 - val_loss: 9.5187e-05\n",
      "Epoch 150/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.7387e-05 - val_loss: 9.5129e-05\n",
      "Epoch 151/1000\n",
      "5703/5703 [==============================] - ETA: 0s - loss: 8.5703e-0 - 0s 21us/step - loss: 8.8160e-05 - val_loss: 9.5945e-05\n",
      "Epoch 152/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8509e-05 - val_loss: 9.4165e-05\n",
      "Epoch 153/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.7451e-05 - val_loss: 9.4939e-05\n",
      "Epoch 154/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8615e-05 - val_loss: 9.5679e-05\n",
      "Epoch 155/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.7271e-05 - val_loss: 9.4025e-05\n",
      "Epoch 156/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.7399e-05 - val_loss: 9.4419e-05\n",
      "Epoch 157/1000\n",
      "5703/5703 [==============================] - 0s 24us/step - loss: 8.7332e-05 - val_loss: 9.8084e-05\n",
      "Epoch 158/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.7599e-05 - val_loss: 9.4300e-05\n",
      "Epoch 159/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 8.6942e-05 - val_loss: 9.5595e-05\n",
      "Epoch 160/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.7954e-05 - val_loss: 9.4853e-05\n",
      "Epoch 161/1000\n",
      "5703/5703 [==============================] - 0s 21us/step - loss: 8.8215e-05 - val_loss: 9.5336e-05\n",
      "Epoch 162/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8402e-05 - val_loss: 9.4329e-05\n",
      "Epoch 163/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.9735e-05 - val_loss: 9.9481e-05\n",
      "Epoch 164/1000\n",
      "5703/5703 [==============================] - 0s 19us/step - loss: 8.9823e-05 - val_loss: 9.3190e-05\n",
      "Epoch 165/1000\n",
      "5703/5703 [==============================] - 0s 23us/step - loss: 8.8527e-05 - val_loss: 9.7929e-05\n",
      "Epoch 166/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8415e-05 - val_loss: 9.6996e-05\n",
      "Epoch 167/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.8106e-05 - val_loss: 9.3442e-05\n",
      "Epoch 168/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.7148e-05 - val_loss: 9.3403e-05\n",
      "Epoch 169/1000\n",
      "5703/5703 [==============================] - 0s 22us/step - loss: 8.7131e-05 - val_loss: 9.3718e-05\n",
      "Epoch 00169: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x83c2737198>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將pastDay 以及futureDay 設為相同長度5\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
