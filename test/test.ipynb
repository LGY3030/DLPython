{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrain():\n",
    "  train = pd.read_csv(\"S&P 500.csv\")\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "  train[\"date\"] = train[\"Date\"].dt.day\n",
    "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "  train = train.drop([\"Date\"], axis=1)\n",
    "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "  return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "  # output shape: (1, 1)\n",
    "  model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 221us/step - loss: 0.0720 - val_loss: 0.0531\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0424 - val_loss: 0.0289\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0202 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 0.0014 - val_loss: 6.0556e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.2921e-04 - val_loss: 3.1463e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.8901e-04 - val_loss: 2.4090e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3611e-04 - val_loss: 2.0371e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.0670e-04 - val_loss: 1.8363e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.8877e-04 - val_loss: 1.7105e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.7593e-04 - val_loss: 1.6137e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 1.6592e-04 - val_loss: 1.5335e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 1.5681e-04 - val_loss: 1.4675e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 1.4872e-04 - val_loss: 1.3966e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 1.4097e-04 - val_loss: 1.3339e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.3401e-04 - val_loss: 1.2662e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 1.2682e-04 - val_loss: 1.2049e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.2079e-04 - val_loss: 1.1464e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.1424e-04 - val_loss: 1.0935e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.0802e-04 - val_loss: 1.0381e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 1.0238e-04 - val_loss: 9.8892e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 9.7147e-05 - val_loss: 9.4579e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 9.2255e-05 - val_loss: 9.0170e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 8.7534e-05 - val_loss: 8.5714e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 8.3437e-05 - val_loss: 8.1836e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 7.9343e-05 - val_loss: 7.8347e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 7.5560e-05 - val_loss: 7.5073e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 7.2313e-05 - val_loss: 7.2008e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 6.9116e-05 - val_loss: 6.9254e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 6.6442e-05 - val_loss: 6.6725e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.3569e-05 - val_loss: 6.4428e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 6.1310e-05 - val_loss: 6.2565e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 5.9211e-05 - val_loss: 6.0566e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.7417e-05 - val_loss: 5.8943e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.5598e-05 - val_loss: 5.7206e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 5.4049e-05 - val_loss: 5.5920e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 5.2564e-05 - val_loss: 5.4755e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 5.1531e-05 - val_loss: 5.3610e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 5.0384e-05 - val_loss: 5.2772e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.9329e-05 - val_loss: 5.2371e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.8701e-05 - val_loss: 5.1376e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.7724e-05 - val_loss: 5.0421e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.6887e-05 - val_loss: 5.0064e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.6379e-05 - val_loss: 4.9356e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.5651e-05 - val_loss: 4.8412e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.5083e-05 - val_loss: 4.7750e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.4619e-05 - val_loss: 4.7322e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.4180e-05 - val_loss: 4.6912e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.3753e-05 - val_loss: 4.6259e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.3211e-05 - val_loss: 4.6237e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.2958e-05 - val_loss: 4.5725e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.2397e-05 - val_loss: 4.5106e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.1983e-05 - val_loss: 4.4408e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.1656e-05 - val_loss: 4.4320e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.1311e-05 - val_loss: 4.3984e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.0903e-05 - val_loss: 4.3549e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 4.0737e-05 - val_loss: 4.3205e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 4.0273e-05 - val_loss: 4.2957e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.9980e-05 - val_loss: 4.2038e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.9706e-05 - val_loss: 4.1925e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.9276e-05 - val_loss: 4.1459e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.9278e-05 - val_loss: 4.1482e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.8572e-05 - val_loss: 4.0678e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.8522e-05 - val_loss: 4.0583e-05\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.7948e-05 - val_loss: 4.0282e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.7863e-05 - val_loss: 3.9675e-05\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.7494e-05 - val_loss: 3.9329e-05\n",
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.7134e-05 - val_loss: 3.9520e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.6952e-05 - val_loss: 3.8466e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.6628e-05 - val_loss: 3.8006e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.6206e-05 - val_loss: 3.8843e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.6181e-05 - val_loss: 3.7545e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.5745e-05 - val_loss: 3.7271e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.5141e-05 - val_loss: 3.7086e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.4982e-05 - val_loss: 3.6381e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.5178e-05 - val_loss: 3.6533e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.4435e-05 - val_loss: 3.6535e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.3994e-05 - val_loss: 3.5517e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.3825e-05 - val_loss: 3.5193e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.3462e-05 - val_loss: 3.4783e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.3421e-05 - val_loss: 3.5254e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2949e-05 - val_loss: 3.4275e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2698e-05 - val_loss: 3.4440e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2702e-05 - val_loss: 3.3376e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2111e-05 - val_loss: 3.3823e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.2027e-05 - val_loss: 3.2771e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.1793e-05 - val_loss: 3.2892e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 3.1582e-05 - val_loss: 3.2670e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.1405e-05 - val_loss: 3.1760e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.0892e-05 - val_loss: 3.2204e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.0992e-05 - val_loss: 3.1905e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0342e-05 - val_loss: 3.1400e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.0225e-05 - val_loss: 3.0810e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.0596e-05 - val_loss: 3.1241e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.9717e-05 - val_loss: 3.0951e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.9429e-05 - val_loss: 2.9968e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.9354e-05 - val_loss: 2.9794e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.8985e-05 - val_loss: 2.9433e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.9123e-05 - val_loss: 2.9387e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8670e-05 - val_loss: 2.9290e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8077e-05 - val_loss: 2.8923e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8028e-05 - val_loss: 2.8740e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.8050e-05 - val_loss: 2.8641e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7742e-05 - val_loss: 2.7913e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7666e-05 - val_loss: 2.7858e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.7467e-05 - val_loss: 2.7560e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7333e-05 - val_loss: 2.8122e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7800e-05 - val_loss: 2.6998e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7100e-05 - val_loss: 2.7472e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6721e-05 - val_loss: 2.7297e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6491e-05 - val_loss: 2.6504e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6664e-05 - val_loss: 2.7892e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.7182e-05 - val_loss: 2.6126e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6217e-05 - val_loss: 2.6909e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.6370e-05 - val_loss: 2.6190e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5696e-05 - val_loss: 2.5687e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5631e-05 - val_loss: 2.5567e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5457e-05 - val_loss: 2.5135e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5394e-05 - val_loss: 2.5898e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5662e-05 - val_loss: 2.6085e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5631e-05 - val_loss: 2.5791e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5737e-05 - val_loss: 2.5945e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.5420e-05 - val_loss: 2.4692e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4904e-05 - val_loss: 2.5333e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5686e-05 - val_loss: 2.4485e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.5014e-05 - val_loss: 2.5413e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4673e-05 - val_loss: 2.4827e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.5276e-05 - val_loss: 2.4187e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4577e-05 - val_loss: 2.4379e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4810e-05 - val_loss: 2.5889e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4686e-05 - val_loss: 2.4284e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4538e-05 - val_loss: 2.3961e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4605e-05 - val_loss: 2.3785e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4447e-05 - val_loss: 2.3782e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4341e-05 - val_loss: 2.4026e-05\n",
      "Epoch 136/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4289e-05 - val_loss: 2.3802e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4663e-05 - val_loss: 2.3298e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4515e-05 - val_loss: 2.3682e-05\n",
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4262e-05 - val_loss: 2.3706e-05\n",
      "Epoch 140/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4301e-05 - val_loss: 2.3777e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4421e-05 - val_loss: 2.3606e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4325e-05 - val_loss: 2.3897e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3925e-05 - val_loss: 2.3474e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3821e-05 - val_loss: 2.2932e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3922e-05 - val_loss: 2.2983e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3878e-05 - val_loss: 2.3389e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4161e-05 - val_loss: 2.2623e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4275e-05 - val_loss: 2.3126e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3715e-05 - val_loss: 2.3894e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3879e-05 - val_loss: 2.3307e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3567e-05 - val_loss: 2.2979e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3987e-05 - val_loss: 2.2632e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3502e-05 - val_loss: 2.2879e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3487e-05 - val_loss: 2.3202e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3946e-05 - val_loss: 2.8491e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.4439e-05 - val_loss: 2.2932e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 9us/step - loss: 2.3866e-05 - val_loss: 2.3807e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4060e-05 - val_loss: 2.3963e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3575e-05 - val_loss: 2.2361e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3600e-05 - val_loss: 2.3716e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3905e-05 - val_loss: 2.4799e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3973e-05 - val_loss: 2.2760e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3382e-05 - val_loss: 2.2684e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3785e-05 - val_loss: 2.3649e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3684e-05 - val_loss: 2.3120e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3302e-05 - val_loss: 2.2197e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3710e-05 - val_loss: 2.1966e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3374e-05 - val_loss: 2.3914e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3906e-05 - val_loss: 2.6167e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.4302e-05 - val_loss: 2.3689e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3891e-05 - val_loss: 2.2342e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3092e-05 - val_loss: 2.2147e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4393e-05 - val_loss: 2.2941e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3558e-05 - val_loss: 2.2429e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3444e-05 - val_loss: 2.6581e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.4085e-05 - val_loss: 2.2637e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3752e-05 - val_loss: 2.2348e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3124e-05 - val_loss: 2.3361e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3890e-05 - val_loss: 2.2598e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3344e-05 - val_loss: 2.2790e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 10us/step - loss: 2.3471e-05 - val_loss: 2.3030e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 2.3924e-05 - val_loss: 2.2016e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf11915630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
